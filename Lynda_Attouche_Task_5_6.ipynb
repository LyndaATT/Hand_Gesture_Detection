{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POu_xiyQ3Y_2"
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------\n",
    "***********************************************************************************************************\n",
    "\n",
    "# <span style=\"color:Purple\"> Computer vision for machine learning Project: \"Detecting hand gestures\"\n",
    "\n",
    "\n",
    "#### Task 5 & 6: \n",
    "\n",
    "#### Author: Lynda Attouche\n",
    "#### Link: https://drive.google.com/file/d/1IoZ_pyLh-EYjCTMZyhkoifGnKLriP7lw/view?usp=sharing\n",
    "*******************************************************************************************\n",
    "----------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7ONPk-v3kd9"
   },
   "source": [
    "## README\n",
    "* Throughout this notebook, no special commands are needed to run the code. Simply run the cells in order. \n",
    "\n",
    "* The code of some tasks loops endlessly, to stop them a counter has been set up. It is possible to comment the counter and so to move on to the next task, just stop it manually and run from the next cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YzuCxsE0syt"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCGHXnOQ0wfS"
   },
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Miav0RSSzBPP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpIUhfv4XWHR",
    "outputId": "4cc4e60d-7c04-4409-9779-058fc1fbb6f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jfVsAD_BUCO"
   },
   "outputs": [],
   "source": [
    "#dataset 1: letters with equal number of pictures and a lot of variability\n",
    "#link: https://drive.google.com/drive/folders/1u9XBwpw8lxd2TtpLyBHlbf9eCqCBLwcC?usp=sharing\n",
    "dataset_path1 = \"/content/drive/MyDrive/ComputerVision/HandGesture_1/\" \n",
    "\n",
    "#dataset 2: letters with unbalanced number of pictures and a lot of variability\n",
    "#link: https://drive.google.com/drive/folders/1TnHp9qpislCBCd6Y8rVctV9eibujBYnm?usp=sharing\n",
    "dataset_path2 = \"/content/drive/MyDrive/ComputerVision/HandGesture_2/\" \n",
    "\n",
    "#dataset 3: letters with equal number of pictures and one of them with no variability \n",
    "#link: https://drive.google.com/drive/folders/1zGyOR2_XicBWWQ0NY-ShuP1co9jnPzZj?usp=sharing\n",
    "dataset_path3 = \"/content/drive/MyDrive/ComputerVision/HandGesture_3/\" \n",
    "path = \"/content/drive/MyDrive/ComputerVision/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF55BEQp02SO"
   },
   "source": [
    "#### OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guocaIf5zQiD",
    "outputId": "c37628e5-c0cf-418c-8cf2-316041cd5ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'opencv'...\n",
      "remote: Enumerating objects: 305186, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 305186 (delta 0), reused 4 (delta 0), pack-reused 305181\u001b[K\n",
      "Receiving objects: 100% (305186/305186), 494.50 MiB | 28.10 MiB/s, done.\n",
      "Resolving deltas: 100% (212371/212371), done.\n",
      "Checking out files: 100% (7044/7044), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/opencv/opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9DRpZTF3jSc"
   },
   "outputs": [],
   "source": [
    "face_cascade_path = \"/content/opencv/data/haarcascades/haarcascade_frontalface_alt.xml\"\n",
    "face_cascades = cv2.CascadeClassifier(face_cascade_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4MkOt5XFhK"
   },
   "source": [
    "### Goal \n",
    "The goal of these two tasks is to generate datasets from the probability map containing the sign presented by the hand after its detection. As it will be detailed a bit more: 4 letters have been chosen (A,E,K,Y) and the 3 datasets are characterized as follows:\n",
    "* dataset 1: letters with equal number of pictures and a lot of variability\n",
    "\n",
    "* dataset 2: letters with unbalanced number of pictures and a lot of variability (K: the minority class, A: majority)\n",
    "\n",
    "* dataset 3: letters with equal number of pictures and one of them with no variability (class K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDHKqF9L4e2C"
   },
   "source": [
    "### Used functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsmCqMoRzWWQ"
   },
   "outputs": [],
   "source": [
    "#Converting image types\n",
    "def byte2image(byte):\n",
    "  jpeg = b64decode(byte.split(',')[1])\n",
    "  im = Image.open(io.BytesIO(jpeg))\n",
    "  return np.array(im)\n",
    "\n",
    "def image2byte(image):\n",
    "  image = Image.fromarray(image)\n",
    "  buffer = io.BytesIO()\n",
    "  image.save(buffer, 'jpeg')\n",
    "  buffer.seek(0)\n",
    "  x = b64encode(buffer.read()).decode('utf-8')\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7IUX2Nz3sT8"
   },
   "outputs": [],
   "source": [
    "def VideoCapture():\n",
    "  js = Javascript('''\n",
    "    async function create(){\n",
    "      div = document.createElement('div'); //create new div element\n",
    "      document.body.appendChild(div); //add the content of the new element to the DOM\n",
    "\n",
    "      video = document.createElement('video'); //create new video element\n",
    "      video.setAttribute('playsinline', ''); //setting attributes of the element\n",
    "\n",
    "      div.appendChild(video); //add the content of video the the div element\n",
    "\n",
    "      //Selecting facing mode of the video stream\n",
    "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
    "      video.srcObject = stream;\n",
    "\n",
    "      await video.play(); //playing video\n",
    "\n",
    "      canvas =  document.createElement('canvas'); //create new canvas element\n",
    "      // set canvas size \n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "\n",
    "      div_out = document.createElement('div'); //create a new div element, will contains the output\n",
    "      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n",
    "      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n",
    "      div_out.appendChild(img); //add the image element to the output div\n",
    "    }\n",
    "\n",
    "    //taking the capture and storing it\n",
    "    async function capture(){\n",
    "        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n",
    "            pendingResolve = resolve;\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n",
    "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
    "            pendingResolve(result);\n",
    "        })\n",
    "    }\n",
    "\n",
    "    //displaying the capture \n",
    "    function showimg(imgb64){\n",
    "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
    "    }\n",
    "\n",
    "  ''')\n",
    "  display(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNTXSei95JTQ"
   },
   "outputs": [],
   "source": [
    "def compute_region(img,margin,prev):\n",
    "  \"\"\"\n",
    "  Computes  region of interest \n",
    "  @params:\n",
    "          - img (array): image on which detection will be done\n",
    "          - margin (int): the margin to be taken from the previous region (the coordinate shift)\n",
    "          - prev (array): previous face detection\n",
    "  @return \n",
    "          - region of interest \n",
    "  \"\"\"\n",
    "  # as seen in the previous task, the cascadeClassifier returns a face as a rectangle\n",
    "  # so this is the case for the param prev, ie: prev = (x,y,w,h) = (prev[0],prev[1],prev[2],prev[3])\n",
    "  # where (x,y) is the top left corner and (w,h) the bottom right corner\n",
    "  # the goal is then to compute (x',y',w',h') considering the margin and previous region to define the new region such that:\n",
    "  # x' = x - margin\n",
    "  # y' = y - margin \n",
    "  # w' = (x+w) + margin\n",
    "  # h' = (y+h) + margin\n",
    "\n",
    "  #top left corner\n",
    "  x_prime = prev[0] - margin \n",
    "  y_prime = prev[1] - margin \n",
    "\n",
    "  #bottom right\n",
    "  w_prime = prev[0]+prev[2]+margin \n",
    "  h_prime = prev[1]+prev[3]+margin \n",
    "\n",
    "  # Note: \n",
    "  #the new region must stay in the image and not be out of it (I noticed that after some tests because the results were weird)\n",
    "  # i.e:\n",
    "  #top left corner should not be negative (as a subtraction is made from the previous x and y) càd:  x_prime>=0 and y_prime>=0\n",
    "  #if either x or y (or both of them) is negative, it should be set to 0\n",
    "  x_prime= max(0,x_prime) # = 0 if x_prime<0\n",
    "  y_prime = max(0,y_prime) #= 0 if y_prime<0\n",
    "\n",
    "  #for the bottom right corner should not be out of the image in the sense that we should not obtain values that go beyond the coordinate of the \n",
    "  #image (since w' for example is a result of increasing x+w with a margin )\n",
    "  #to handle that possible problem, the value should be set to the image height or width (depends on which coordinate) of the image, as follows\n",
    "  (imgH,imgW) = img.shape[0], img.shape[1]\n",
    "  w_prime = min(imgW,w_prime) # = image width if x_prime>image width\n",
    "  h_prime = min(imgH,h_prime) # = image heigh if y_prime>image heigh\n",
    "\n",
    "  return (x_prime,y_prime,w_prime-x_prime,h_prime-y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MrBVMr5ohbA"
   },
   "outputs": [],
   "source": [
    "def detect_face(im,prev,margin):\n",
    "  \"\"\"\n",
    "  Detects face regions\n",
    "  @param:\n",
    "        - im (array): image/capture\n",
    "        - prev (array): previous detected area\n",
    "        - margin (int): margin for detection \n",
    "  @return \n",
    "        detected region w/out margin \n",
    "  \"\"\"\n",
    "  new_reg = im.copy() #starter region, whole image (first step, step t before detection)\n",
    "  curr_face = None #contains the detected face\n",
    "  #if we didn't detect a face yet\n",
    "  if prev is not None :\n",
    "    #print(\"I am here\")\n",
    "    x_prime,y_prime,w_prime,h_prime = compute_region(im,margin,prev) #computing the new region (of timestep t+1)\n",
    "    #my new region:\n",
    "    new_reg = im[y_prime:y_prime+h_prime,x_prime:x_prime+w_prime] \n",
    "    #plot the rectangle \n",
    "    cv2.rectangle(im, (x_prime,y_prime), (x_prime+w_prime, y_prime+h_prime), (255,0,0),2)\n",
    "\n",
    "  gray = cv2.cvtColor(new_reg, cv2.COLOR_BGR2GRAY) # Converting image to gray scale\n",
    "  #face detection using face cascade \n",
    "  faces = face_cascades.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=4)\n",
    "  if len(faces)==0: # no face has been detected \n",
    "    prev = None  #we won't have a roi *\n",
    "  else:\n",
    "    curr_face = faces[0] #we pick the first face detected\n",
    "    if prev is None: #previous region surrounding the face picked\n",
    "      (x,y,w,h) = curr_face #get the face region coordinates\n",
    "    else: #we already have a face, so we have already computed xprime,yprime,\n",
    "      (x,y,w,h)=(x_prime+curr_face[0], y_prime+curr_face[1], curr_face[2], curr_face[3]) #updating with the new coordinates (primes)\n",
    "    #so we update our region \n",
    "    prev = (x,y,w,h)\n",
    "    #we plot the rectangle \n",
    "    cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0),2)    \n",
    "  return curr_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvVgcHfJ9_ee"
   },
   "outputs": [],
   "source": [
    "def hand_position(pts):\n",
    "  \"\"\"\n",
    "  Computing the position of the box around the hand\n",
    "  @params:\n",
    "          pts: the box points (points in corners)\n",
    "  @return:\n",
    "          position of the hand\n",
    "  \"\"\"\n",
    "  #in this following lines of code, we'll be take either the min or max so we can control the position of hand\n",
    "  #insuring that the hands stays insides the image\n",
    "  x_top_l = max(0, min(pts[:,0]))\n",
    "  y_top_l = max(0, min(pts[:,1]))\n",
    "  x_bottom_r = min(im.shape[1], max(pts[:,0]))\n",
    "  y_bottom_r = min(im.shape[0],  max(pts[:,1 ]))\n",
    "  hand =  (x_top_l, y_top_l), (x_bottom_r, y_bottom_r)\n",
    "  return hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OkIMiYmCLQL"
   },
   "outputs": [],
   "source": [
    "def save_hand(dataset_path,prob, hand_box,letter,res_dim,num):\n",
    "  \"\"\"\n",
    "  Saves image into colab folder\n",
    "  @params: \n",
    "          prob : probability map containing hand\n",
    "          hand_box : hand position \n",
    "          letter: letter represented by the hand gesture\n",
    "          res_dim: dimension of saving\n",
    "          num: number/index of the image\n",
    "  @return:\n",
    "          saved image\n",
    "  \"\"\"\n",
    "\n",
    "  hand = prob[hand_box[0][1]:hand_box[1][1], hand_box[0][0]:hand_box[1][0]]\n",
    "  hand = cv2.resize(hand, (res_dim,res_dim)) #resizing the image to given dimension\n",
    "  filename = dataset_path +'/'+letter+'/'+letter+'_'+str(num)+'_'+str(res_dim)+'.jpg' #naming the image\n",
    "  return cv2.imwrite(filename, hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5LOunlQ-miS"
   },
   "outputs": [],
   "source": [
    "def save_image(dataset_path,im,letter,res_dim,num):\n",
    "  \"\"\"\n",
    "  Saves image into colab folder\n",
    "  @params: \n",
    "          im : image to save\n",
    "          letter: letter represented by the hand gesture\n",
    "          res_dim: dimension of saving\n",
    "          num: number/index of the image\n",
    "  @return:\n",
    "          saved image\n",
    "  \"\"\"\n",
    "  filename = dataset_path +'/'+letter+'/'+letter+'_'+str(num)+'_'+str(res_dim)+'.jpg' #naming the image\n",
    "  return cv2.imwrite(filename, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZXHCyYy-HRH"
   },
   "outputs": [],
   "source": [
    "def to_txt(path,dataset_path,num_dataset):\n",
    "  \"\"\"\n",
    "  Writes images in txt file\n",
    "  @params:\n",
    "          path: path to the file\n",
    "          dataset_path : path to images\n",
    "  @return:\n",
    "          txt file in colab with all images written on it\n",
    "  \"\"\"\n",
    "  images_y = os.listdir(dataset_path+'y/') #reading folder containing images of label y\n",
    "  images_a = os.listdir(dataset_path+'a/') #reading folder containing images of label a\n",
    "  images_e = os.listdir(dataset_path+'e/') #reading folder containing images of label e\n",
    "  images_k = os.listdir(dataset_path+'k/') #reading folder containing images of label k\n",
    "  with open(path+'dataset'+str(num_dataset)+'.txt','w') as f: #writing on the file dataset\n",
    "\n",
    "    for im_ in images_y: #for each image of label Y\n",
    "      im = cv2.imread(dataset_path+'y/'+im_,cv2.IMREAD_GRAYSCALE) #reading current image\n",
    "      if im.shape[0] == 16: #selecting only images of shape 16\n",
    "        im = im.reshape((1,256)) #reshaping current image of size 16 to (1,256)\n",
    "        f.write(''.join('Y,'))\n",
    "        np.savetxt(f, im, delimiter=',', fmt='%d') #writing image on the file \n",
    "        \n",
    "    for im_ in images_a: #for each image of label A\n",
    "      im = cv2.imread(dataset_path+'a/'+im_,cv2.IMREAD_GRAYSCALE)\n",
    "      if im.shape[0] == 16:\n",
    "        im = im.reshape((1,256))\n",
    "        f.write(''.join('A,'))\n",
    "        np.savetxt(f, im, delimiter=',', fmt='%d')\n",
    "\n",
    "    for im_ in images_e: #for each image of label E\n",
    "      im = cv2.imread(dataset_path+'e/'+im_,cv2.IMREAD_GRAYSCALE)\n",
    "      if im.shape[0] == 16:\n",
    "        im = im.reshape((1,256))\n",
    "        f.write(''.join('E,'))\n",
    "        np.savetxt(f, im, delimiter=',', fmt='%d')\n",
    "    \n",
    "    for im_ in images_k: #for each image of label K\n",
    "      im = cv2.imread(dataset_path+'k/'+im_,cv2.IMREAD_GRAYSCALE)\n",
    "      if im.shape[0] == 16:\n",
    "        im = im.reshape((1,256))\n",
    "        f.write(''.join('K,'))\n",
    "        np.savetxt(f, im, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBqcXzay-Msi"
   },
   "outputs": [],
   "source": [
    "def transformations(dataset_path,image,letter,res_dim,num):\n",
    "  \"\"\"\n",
    "  Applies a specific transformation on a given image and saves it\n",
    "  @params:\n",
    "          image : image on which we apply transformation\n",
    "          letter: letter represented by the hand gesture\n",
    "          res_dim: dimension of saving\n",
    "          num: number/index of the image\n",
    "\n",
    "  @return:\n",
    "          transformed and saved image (array) \n",
    "          last index (int)\n",
    "  \"\"\"\n",
    "  width, height = image.shape\n",
    "\n",
    "  #copy the initial image\n",
    "  transformed_image1 = np.copy(image)\n",
    "  #rotation matrix 2x3:\n",
    "  #((height-1)/2.0,(width-1)/2.0) is the center of rotation and here it is the center of image\n",
    "  #the angle is 180°\n",
    "  #1 indicates that the scale is not changed\n",
    "  Mrot = cv2.getRotationMatrix2D(((height-1)/2.0,(width-1)/2.0),180,1) \n",
    "  #here an affine transformation(matrix) used to project the rotated image\n",
    "  transformed_image1 = cv2.warpAffine(transformed_image1,Mrot,(height,width))\n",
    "  save_image(dataset_path,transformed_image1,letter,res_dim,num+1)\n",
    "\n",
    "  transformed_image2 = np.copy(image)\n",
    "  #rotation matrix 2x3:\n",
    "  #((height-1)/2.0,(width-1)/2.0) is the center of rotation and here it is the center of image\n",
    "  #the angle is 90°\n",
    "  #1 indicates that the scale is not changed\n",
    "  Mrot = cv2.getRotationMatrix2D(((height-1)/2.0,(width-1)/2.0),90,1) \n",
    "  #here an affine transformation(matrix) used to project the rotated image\n",
    "  transformed_image2 = cv2.warpAffine(transformed_image2,Mrot,(height,width))\n",
    "  save_image(dataset_path,transformed_image2,letter,res_dim,num+2)\n",
    "  \n",
    "  transformed_image3 = np.copy(image)\n",
    "  #rotation matrix 2x3:\n",
    "  #((height-1)/2.0,(width-1)/2.0) is the center of rotation and here it is the center of image\n",
    "  #the angle is -90°\n",
    "  #1 indicates that the scale is not changed\n",
    "  Mrot = cv2.getRotationMatrix2D(((height-1)/2.0,(width-1)/2.0),-90,1) \n",
    "  #here an affine transformation(matrix) used to project the rotated image\n",
    "  transformed_image3 = cv2.warpAffine(transformed_image3,Mrot,(height,width))\n",
    "  save_image(dataset_path,transformed_image3,letter,res_dim,num+3)\n",
    "\n",
    "  transformed_image4 = np.copy(image)\n",
    "  #rotation matrix 2x3:\n",
    "  #((height-1)/2.0,(width-1)/2.0) is the center of rotation and here it is the center of image\n",
    "  #the angle is 60°\n",
    "  #1 indicates that the scale is not changed\n",
    "  Mrot = cv2.getRotationMatrix2D(((height-1)/2.0,(width-1)/2.0),60,1) \n",
    "  #here an affine transformation(matrix) used to project the rotated image\n",
    "  transformed_image4 = cv2.warpAffine(transformed_image4,Mrot,(height,width))\n",
    "  save_image(dataset_path,transformed_image4,letter,res_dim,num+4)\n",
    "\n",
    "  transformed_image5 = np.copy(image)\n",
    "  #rotation matrix 2x3:\n",
    "  #((height-1)/2.0,(width-1)/2.0) is the center of rotation and here it is the center of image\n",
    "  #the angle is -60°\n",
    "  #1 indicates that the scale is not changed\n",
    "  Mrot = cv2.getRotationMatrix2D(((height-1)/2.0,(width-1)/2.0),-60,1) \n",
    "  #here an affine transformation(matrix) used to project the rotated image\n",
    "  transformed_image5 = cv2.warpAffine(transformed_image5,Mrot,(height,width))\n",
    "  save_image(dataset_path,transformed_image5,letter,res_dim,num+5)\n",
    "\n",
    "  transformed_image6= np.copy(image)\n",
    "  #3pts(pts1) and their transforms are used to define the matrix of affine transformation\n",
    "  pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "  pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "  #the obtained matrix:\n",
    "  Maff = np.float32([[1,0,100],[0,1,50]]) \n",
    "  #project the image after affine trasformation\n",
    "  transformed_image6 = cv2.warpAffine(transformed_image6,Maff,(height,width))\n",
    "  save_image(dataset_path,transformed_image5,letter,res_dim,num+6)\n",
    "\n",
    "  transformed_image7= np.copy(image)\n",
    "  #translating image\n",
    "  quarter_height, quarter_width = height / 4, width / 4\n",
    "  T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n",
    "  #using warpAffine to transform\n",
    "  #translating image using the translation matrix T\n",
    "  transformed_image7 = cv2.warpAffine(transformed_image7, T, (width, height))\n",
    "\n",
    "  save_image(dataset_path,transformed_image5,letter,res_dim,num+7)\n",
    "  return num+7 #returns last index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I42WWCmZjABC"
   },
   "outputs": [],
   "source": [
    "def apply_transformation(dataset_path,c_y,c_a,c_e,c_k):\n",
    "  \"\"\"\n",
    "  Apply transformation for each image in each folder\n",
    "  @params\n",
    "          c_y (int): last index in folder y\n",
    "          c_a (int): last index in folder a\n",
    "          c_e (int): last index in folder e\n",
    "          c_k (int): last index in folder k\n",
    "  @return \n",
    "          saved transformed images\n",
    "  \"\"\"\n",
    "  images_y = os.listdir(dataset_path+'y/') #reading content of folder y \n",
    "  images_y = [cv2.imread(dataset_path+'y/'+im,cv2.IMREAD_GRAYSCALE) for im in images_y] #reading images of label y\n",
    "  images_y_16 = [im for im in images_y if im.shape[0]==16] #contains images of label Y size 16x16\n",
    "  images_y_224 = [im for im in images_y if im.shape[0]==224] #contains images of label Y of size 224x224\n",
    "\n",
    "  images_a = os.listdir(dataset_path+'a/') #reading content of folder a\n",
    "  images_a = [cv2.imread(dataset_path+'a/'+im,cv2.IMREAD_GRAYSCALE) for im in images_a] #reading images of label a\n",
    "  images_a_16 = [im for im in images_a if im.shape[0]==16] #contains images of label A size 16x16\n",
    "  images_a_224 = [im for im in images_a if im.shape[0]==224] #contains images of label A of size 224x224\n",
    "\n",
    "\n",
    "  images_e = os.listdir(dataset_path+'e/') #reading content of folder e\n",
    "  images_e = [cv2.imread(dataset_path+'e/'+im,cv2.IMREAD_GRAYSCALE) for im in images_e] #reading images of label e\n",
    "  images_e_16 = [im for im in images_e if im.shape[0]==16] #contains images of label E size 16x16\n",
    "  images_e_224 = [im for im in images_e if im.shape[0]==224] #contains images of label E of size 224x224\n",
    "\n",
    "  images_k = os.listdir(dataset_path+'k/') #reading content of folder k\n",
    "  images_k = [cv2.imread(dataset_path+'k/'+im,cv2.IMREAD_GRAYSCALE) for im in images_k] #reading images of label k\n",
    "  images_k_16 = [im for im in images_k if im.shape[0]==16] #contains images of label K size 16x16\n",
    "  images_k_224 = [im for im in images_k if im.shape[0]==224] #contains images of label K of size 224x224\n",
    "\n",
    "  c__a,c__y,c__e,c__k = c_a,c_y,c_e,c_k #taking track of images indices \n",
    "  \n",
    "  #Label: Y\n",
    "  for im in images_y_16: #for each image of size 16x16\n",
    "    c_y = transformations(dataset_path,im,'y',16,c_y) #applying & saving transformations\n",
    "  c_y = c__y #initial value\n",
    "  for im in images_y_224: #for each image of size 224x224\n",
    "    c_y = transformations(dataset_path,im,'y',224,c_y) #applying & saving transformations\n",
    "\n",
    "  #Label: A\n",
    "  for im in images_a_16: #for each image of size 16x16\n",
    "    c_a = transformations(dataset_path,im,'a',16,c_a) #applying & saving transformations\n",
    "  c_a = c__a #initial value\n",
    "  for im in images_a_224: #for each image of size 224x224\n",
    "    c_a = transformations(dataset_path,im,'a',224,c_a)  #applying & saving transformations\n",
    "\n",
    "  #Label: E\n",
    "  for im in images_e_16: #for each image of size 16x16\n",
    "    c_e = transformations(dataset_path,im,'e',16,c_e) #applying & saving transformations\n",
    "  c_e = c__e  #initial value\n",
    "  for im in images_e_224: #for each image of size 224x224\n",
    "    c_e = transformations(dataset_path,im,'e',224,c_e) #applying & saving transformations\n",
    "\n",
    "  #Label: K\n",
    "  for im in images_k_16: #for each image of size 16x16\n",
    "    c_k = transformations(dataset_path,im,'k',16,c_k) #applying & saving transformations\n",
    "  c_k = c__k  #initial value\n",
    "  for im in images_k_224: #for each image of size 224x224\n",
    "    c_k = transformations(dataset_path,im,'k',224,c_k)  #applying & saving transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9sCXC4OAEPD"
   },
   "source": [
    "#### 1. Face detection\n",
    "The first step is the detection of the face to be able to calculate the corresponding histogram and make a back projection on the new captures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbGTsrMh5HfP"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "\n",
    "#out first starting box: \n",
    "prev = None \n",
    "margin = 50 #randomly chosen\n",
    "b = True\n",
    "c = 0 #counter just to stop the algorithm, since it takes time (infinit loop)\n",
    "while b:\n",
    "  byte = eval_js('capture()')\n",
    "  im = byte2image(byte) \n",
    "  curr_face = detect_face(im,prev,margin)\n",
    "   \n",
    "  if c>20:\n",
    "    #in this part, I decided to display only the face frame so I can play with the distance to the camera and I didn't display the big rectangle\n",
    "    #around, if you want to display it, you just have to return \"prev\" from the fuction that computes the face detection and display it using:\n",
    "    #eval_js('showimg(\"{}\")'.format(image2byte(im[prev[1]:prev[1]+prev[3], prev[0]:prev[0]+prev[2]])))\n",
    "    face_frame = im[curr_face[1]:curr_face[1]+curr_face[3], curr_face[0]:curr_face[0]+curr_face[2]]\n",
    "    tracking_window_face = curr_face\n",
    "    eval_js('showimg(\"{}\")'.format(image2byte(face_frame)))\n",
    "    break\n",
    "  c +=1\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_lGriWgAYsS"
   },
   "source": [
    "#### 2. Computing histogram for detected face\n",
    "\n",
    "After the detection of the face, the calculation of the histogram is done as follows. This one will be backpropagated to be able to detect on a given capture the different elements of the same color as the face, in this case, the hands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5u9JmiHemAYh"
   },
   "outputs": [],
   "source": [
    "def hsv(face_frame):\n",
    "  \"\"\"\n",
    "  Transforms the face frame into HSV and computes histogram\n",
    "  @params:\n",
    "        - face_frame: the frame representing the face\n",
    "  @return:\n",
    "        - mask: hsv with mask to deal with brightness and darkness pixels\n",
    "        - histo: histogram computed from the hsv\n",
    "  \"\"\"\n",
    "  #transforming the detected face frame into HSV\n",
    "  hsv = cv2.cvtColor(face_frame, cv2.COLOR_BGR2HSV)\n",
    "  #Creating a mask using inRange for the pixels to deal with brightness and darkness\n",
    "  #allows us to take into consideration the pixels that are too dark and/or too bright\n",
    "  #the parameters of the mask were found just by playing and testing many of values (the tests were held in one only place)\n",
    "  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n",
    "  #computing histogram of face frame using hue channel ie [0]\n",
    "  #here we used: 18 as bins for the histogram\n",
    "  #and the range of the hue was set to [0,180]\n",
    "  histo = cv2.calcHist([hsv],[0], mask, [18], [0,180])\n",
    "  #normalizing histogram 0-255\n",
    "  histo = cv2.normalize(histo, histo, 0, 255, cv2.NORM_MINMAX)\n",
    "  return mask, hsv,histo\n",
    "\n",
    "mask,hsv,histo = hsv(face_frame)\n",
    "#Displaying histogram\n",
    "plt.imshow(histo.reshape(1,-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HXhMkp36CbA"
   },
   "source": [
    "#### 3. Datecting hand and saving images\n",
    "In this step, the probability map containing the hand is stored each time. It was decided to set the threshold to 20. This means that for each letter 20 probability maps containing the chosen letter are stored. The size of the dataset will be increased in the next step. \n",
    "For each letter, we store the \"hand\" in dataset 1 with sizes 16 and 224. Respecting the name of the image as it is: letter_index_size. \n",
    "\n",
    "* For dataset 1: 20 captures were made with variations of the hand (front, profile, upside down, translate,..). \n",
    "\n",
    "* For dataset 2: no capture was required, as it is the same as dataset 1 with different sizes for the classes.\n",
    "\n",
    "* For dataset 3: the captures of the letters a, e, and y were used. And for the letter k, 20 captures were taken without variation, see some of the images were identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFXko2EF1yjo"
   },
   "outputs": [],
   "source": [
    "VideoCapture()\n",
    "eval_js('create()')\n",
    "#this following line describes a criteria to stop camshift algorithm\n",
    "#so this algorithm stops when 10 iterations have been carried out or when the computed value is not changing in all the direction by a factor of 1pt \n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "tracking_window_hand = (0,0,im.shape[1],im.shape[0]) #to keep track of the hand\n",
    "c = 0 #couter to stop the algo\n",
    "c_A, c_E, c_K, c_Y = 0,0,0,0\n",
    "letter = input(\"Letter?\")\n",
    "while True:\n",
    "  time.sleep(2)\n",
    "  byte = eval_js('capture()') # capture\n",
    "  im = byte2image(byte) #converting capture \n",
    "  # Converting the image to HSV\n",
    "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "  # Computing mask (inRange) as done in the previous cell\n",
    "  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n",
    "  # Back projecting the face frame histogram into the hsv image\n",
    "  #basically, we have the histogram of colors of the face and we will backproject it in our current image \n",
    "  #to detect the part of the image that fit the histogram (have same color as the face)\n",
    "  prob = cv2.calcBackProject([hsv],[0], histo, [0,180],scale= 1)\n",
    "  \n",
    "  # Apply the mask to the backprojection output\n",
    "  # Helps us to deal with dark or/and bright pixels\n",
    "  prob = prob & mask\n",
    "\n",
    "  #Tracking face \n",
    "  # Applying cam shift\n",
    "  (x,y,w,h) = tracking_window_face\n",
    "  ret,tracking_window_face = cv2.CamShift(prob,tracking_window_face, term_crit)\n",
    "  # Retrieve the rotated bounding rectangle\n",
    "  pts = cv2.boxPoints(ret).astype(np.int)\n",
    "  # fill the face area (prob) with zeros\n",
    "  cv2.fillPoly(prob, [pts], 0)\n",
    "  # Draw the face area\n",
    "  cv2.polylines(im, [pts], True, (255, 255 , 0), 2)\n",
    " \n",
    "  #Tracking hand\n",
    "  ret2, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n",
    "  \n",
    "  pts2 = cv2.boxPoints(ret2).astype(np.int)\n",
    "  hand = hand_position(pts2)\n",
    "\n",
    "  #drawing the rectangle around hand \n",
    "  cv2.rectangle(im, hand[0], hand[1], (0,0,255), 2)\n",
    "\n",
    "  time.sleep(1) #wait a moment \n",
    "\n",
    "  if letter == 'a': #if sign is A\n",
    "    if c_A >20: #taking only 20 images\n",
    "      break\n",
    "    save_hand(dataset_path1,prob,hand,'a',16,c_A) #saving probability map with size 16x16\n",
    "    save_hand(dataset_path1,prob,hand,'a',224,c_A) #saving probability map size 224x224\n",
    "    c_A +=1\n",
    "\n",
    "  elif letter == 'e': #if sign is E\n",
    "    if c_E >20: #taking only 20 images\n",
    "      break\n",
    "    save_hand(dataset_path1,prob,hand,'e',16,c_E) #saving probability map size 16x16\n",
    "    save_hand(dataset_path1,prob,hand,'e',224,c_E) #saving probability map size 224x224\n",
    "    c_E +=1\n",
    "  elif letter == 'k' : #if sign is K\n",
    "    if c_K >20: #taking only 20 images\n",
    "      break\n",
    "    save_hand(dataset_path3,prob,hand,'k',16,c_K) #saving probability map with size 16x16\n",
    "    save_hand(dataset_path3,prob,hand,'k',224,c_K) #saving probability map with size 224x224\n",
    "    c_K +=1\n",
    "\n",
    "  elif letter == 'y': #if sign is Y\n",
    "    if c_Y >20: #taking only 20 images\n",
    "      break\n",
    "    save_hand(dataset_path1,prob,hand,'y',16,c_Y) #saving probability map with size 16x16\n",
    "    save_hand(dataset_path1,prob,hand,'y',224,c_Y) #saving probability map with size 224x224\n",
    "    c_Y +=1\n",
    "\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
    "  eval_js('showimg(\"{}\")'.format(image2byte(prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUiOT1gp6JZc"
   },
   "source": [
    "#### 4. Data augmentation for 1st dataset using OpenCV\n",
    "\n",
    "Previously, 20 video captures were taken. However, a dataset of at least 100 images per class was required. \n",
    "Now, to increase the size of the dataset, geometrical transformations (rotation with 180, 90, and 60 angles and translations) were applied to each image (of each class) which are detailed in transformation function above and that uses OpenCV library.\n",
    "This allowed us to go from 20 images per label to 168 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuNN-AAw4Eou"
   },
   "outputs": [],
   "source": [
    "c_y,c_a,c_e,c_k = 20,20,20,20 #indices from which new data will be stored \n",
    "#applying transformation on saved images using geometric transformation described in the function\n",
    "apply_transformation(dataset_path1,c_y,c_a,c_e,c_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rootg9B3XFiw"
   },
   "source": [
    "For the second dataset, the k class was reduced to 50 images. The classes e and y were truncated to 100 data. And finally, the class represents the majority class with 168 images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh5WDThl6PLM"
   },
   "source": [
    "#### 5. Increasing 3rd dataset size by duplicating images\n",
    "For the case of this dataset, a non-variability of the k class was necessary. So, in order to respect this and to have the same number of data as the other classes, a duplication of the images was made. Indeed, each image among the 20 was duplicated a certain number of times to reach the desired size (168 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jnb-f4HAAhU"
   },
   "outputs": [],
   "source": [
    "images_k = os.listdir(dataset_path3+'k/') #reading content of folder of label k in 3rd dataset\n",
    "images_k = [cv2.imread(dataset_path3+'k/'+im,cv2.IMREAD_GRAYSCALE) for im in images_k] #reading images of 3rd dataset of label K\n",
    "images_k_16 = [im for im in images_k if im.shape[0]==16] #contains images of size 16x16\n",
    "images_k_224 = [im for im in images_k if im.shape[0]==224] #contains images of size 224x224\n",
    "#this following steps have been done twice: first from index 20 and 84\n",
    "c_k, c__k = 20,20  #indices from which new images will be stored \n",
    "for imk in images_k_16: #for each image of size 16x16\n",
    "  save_image(dataset_path3,imk,'k',16,c_k) #duplicating current image\n",
    "  c_k +=1\n",
    "c_k = c__k\n",
    "for imk in images_k_224: #for each image of size 224x224\n",
    "  save_image(dataset_path3,imk,'k',224,c_k) #duplicating current image\n",
    "  c_k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDBRWx1m6qIV"
   },
   "source": [
    "#### 6. Text file saving and shuffling data\n",
    "Now that the datasets are ready, text files have been generated for each of them. They contain the data (images) where each line represents the image and its label. They will be used in the model phase (next task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRpt9BQXdClv"
   },
   "outputs": [],
   "source": [
    "#dataset 1\n",
    "to_txt(path,dataset_path1,1) #generating text file for 1st dataset\n",
    "#shuffling rows of generated text file\n",
    "lines = open(path+'dataset1.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open(path+'dataset1.txt', 'w').writelines(lines)\n",
    "\n",
    "#dataset 2\n",
    "to_txt(path,dataset_path2,2) #generating text file for 2nd dataset\n",
    "#shuffling rows of generated text file\n",
    "lines = open(path+'dataset2.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open(path+'dataset2.txt', 'w').writelines(lines)\n",
    "\n",
    "#dataset 3\n",
    "to_txt(path,dataset_path3,3) #generating text file for 3rd dataset\n",
    "#shuffling rows of generated text file\n",
    "lines = open(path+'dataset3.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open(path+'dataset3.txt', 'w').writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lynda_Attouche_Task_5_6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
