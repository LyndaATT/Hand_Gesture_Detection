{"cells":[{"cell_type":"markdown","source":["----------------------------------------------------------------------------------------------------------\n","***********************************************************************************************************\n","\n","# <span style=\"color:Purple\"> Computer vision for machine learning Project: \"Detecting hand gestures\"\n","\n","\n","#### Task 3 & 4: \n","\n","#### Author: Lynda Attouche\n","#### Link: https://colab.research.google.com/drive/1nb6DTYYut70uQ47FPm41KzbI-S9OIwzZ?usp=sharing\n","*******************************************************************************************\n","----------------------------------------------------------------------------------------------------------\n"],"metadata":{"id":"BWKpc5bG0G5X"}},{"cell_type":"markdown","source":["## README\n","* Throughout this notebook, no special commands are needed to run the code. Simply run the cells in order. \n","\n","* The code of some tasks loops endlessly, to stop them a counter has been set up. It is possible to comment the counter and so to move on to the next task, just stop it manually and run from the next cell "],"metadata":{"id":"yBcvgIID0Tr4"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"2YzuCxsE0syt"}},{"cell_type":"markdown","source":["#### Libraries"],"metadata":{"id":"bCGHXnOQ0wfS"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Miav0RSSzBPP","executionInfo":{"status":"ok","timestamp":1648826210272,"user_tz":-120,"elapsed":563,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"outputs":[],"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import numpy as np\n","from PIL import Image\n","import io\n","import cv2\n","from matplotlib import pyplot as plt\n","import time"]},{"cell_type":"markdown","source":["#### OpenCV"],"metadata":{"id":"cF55BEQp02SO"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43842,"status":"ok","timestamp":1648826254097,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"},"user_tz":-120},"id":"guocaIf5zQiD","outputId":"429db13e-7295-4ef5-dbe3-ea65698f7b88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'opencv'...\n","remote: Enumerating objects: 304494, done.\u001b[K\n","remote: Total 304494 (delta 0), reused 0 (delta 0), pack-reused 304494\u001b[K\n","Receiving objects: 100% (304494/304494), 493.72 MiB | 22.14 MiB/s, done.\n","Resolving deltas: 100% (211892/211892), done.\n","Checking out files: 100% (7038/7038), done.\n"]}],"source":["!git clone https://github.com/opencv/opencv/"]},{"cell_type":"markdown","metadata":{"id":"RDHKqF9L4e2C"},"source":["### Helper functions\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tsmCqMoRzWWQ","executionInfo":{"status":"ok","timestamp":1648826254102,"user_tz":-120,"elapsed":40,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"outputs":[],"source":["#Converting image types\n","def byte2image(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def image2byte(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Y9DRpZTF3jSc","executionInfo":{"status":"ok","timestamp":1648826254103,"user_tz":-120,"elapsed":36,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"outputs":[],"source":["face_cascade_path = \"/content/opencv/data/haarcascades/haarcascade_frontalface_alt.xml\"\n","face_cascades = cv2.CascadeClassifier(face_cascade_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"x7IUX2Nz3sT8","executionInfo":{"status":"ok","timestamp":1648826254106,"user_tz":-120,"elapsed":37,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"outputs":[],"source":["def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div'); //create new div element\n","      document.body.appendChild(div); //add the content of the new element to the DOM\n","\n","      video = document.createElement('video'); //create new video element\n","      video.setAttribute('playsinline', ''); //setting attributes of the element\n","\n","      div.appendChild(video); //add the content of video the the div element\n","\n","      //Selecting facing mode of the video stream\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play(); //playing video\n","\n","      canvas =  document.createElement('canvas'); //create new canvas element\n","      // set canvas size \n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div'); //create a new div element, will contains the output\n","      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n","      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n","      div_out.appendChild(img); //add the image element to the output div\n","    }\n","\n","    //taking the capture and storing it\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n","            result = canvas.toDataURL('image/jpeg', 0.8);\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    //displaying the capture \n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)"]},{"cell_type":"markdown","source":["In the following cell, a function has been defined in order to calculate the region on which the face search will be carried out at time **t+1** taking into account the previous face delimitation at time **t** (if we refer to the figure on moodle)"],"metadata":{"id":"EzWs9L621SfV"}},{"cell_type":"code","source":["def compute_region(img,margin,prev):\n","  \"\"\"\n","  Computes  region of interest \n","  @params:\n","          - img (array): image on which detection will be done\n","          - margin (int): the margin to be taken from the previous region (the coordinate shift)\n","          - prev (array): previous face detection\n","  @return \n","          - region of interest \n","  \"\"\"\n","  # as seen in the previous task, the cascadeClassifier returns a face as a rectangle\n","  # so this is the case for the param prev, ie: prev = (x,y,w,h) = (prev[0],prev[1],prev[2],prev[3])\n","  # where (x,y) is the top left corner and (w,h) the bottom right corner\n","  # the goal is then to compute (x',y',w',h') considering the margin and previous region to define the new region such that:\n","  # x' = x - margin\n","  # y' = y - margin \n","  # w' = (x+w) + margin\n","  # h' = (y+h) + margin\n","\n","  #top left corner\n","  x_prime = prev[0] - margin \n","  y_prime = prev[1] - margin \n","\n","  #bottom right\n","  w_prime = prev[0]+prev[2]+margin \n","  h_prime = prev[1]+prev[3]+margin \n","\n","  # Note: \n","  #the new region must stay in the image and not be out of it (I noticed that after some tests because the results were weird)\n","  # i.e:\n","  #top left corner should not be negative (as a subtraction is made from the previous x and y) cÃ d:  x_prime>=0 and y_prime>=0\n","  #if either x or y (or both of them) is negative, it should be set to 0\n","  x_prime= max(0,x_prime) # = 0 if x_prime<0\n","  y_prime = max(0,y_prime) #= 0 if y_prime<0\n","\n","  #for the bottom right corner should not be out of the image in the sense that we should not obtain values that go beyond the coordinate of the \n","  #image (since w' for example is a result of increasing x+w with a margin )\n","  #to handle that possible problem, the value should be set to the image height or width (depends on which coordinate) of the image, as follows\n","  (imgH,imgW) = img.shape[0], img.shape[1]\n","  w_prime = min(imgW,w_prime) # = image width if x_prime>image width\n","  h_prime = min(imgH,h_prime) # = image heigh if y_prime>image heigh\n","\n","  return (x_prime,y_prime,w_prime-x_prime,h_prime-y_prime)"],"metadata":{"id":"tNTXSei95JTQ","executionInfo":{"status":"ok","timestamp":1648826254110,"user_tz":-120,"elapsed":40,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def detect_face(im,prev,margin):\n","  \"\"\"\n","  Detects face regions\n","  @param:\n","        - im (array): image/capture\n","        - prev (array): previous detected area\n","        - margin (int): margin for detection \n","  @return \n","        detected region w/out margin \n","  \"\"\"\n","  new_reg = im.copy() #starter region, whole image (first step, step t before detection)\n","  curr_face = None #contains the detected face\n","  #if we didn't detect a face yet\n","  if prev is not None :\n","    #print(\"I am here\")\n","    x_prime,y_prime,w_prime,h_prime = compute_region(im,margin,prev) #computing the new region (of timestep t+1)\n","    #my new region:\n","    new_reg = im[y_prime:y_prime+h_prime,x_prime:x_prime+w_prime] \n","    #plot the rectangle \n","    cv2.rectangle(im, (x_prime,y_prime), (x_prime+w_prime, y_prime+h_prime), (255,0,0),2)\n","\n","  gray = cv2.cvtColor(new_reg, cv2.COLOR_BGR2GRAY) # Converting image to gray scale\n","  #face detection using face cascade \n","  faces = face_cascades.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=4)\n","  if len(faces)==0: # no face has been detected \n","    prev = None  #we won't have a roi *\n","  else:\n","    curr_face = faces[0] #we pick the first face detected\n","    if prev is None: #previous region surrounding the face picked\n","      (x,y,w,h) = curr_face #get the face region coordinates\n","    else: #we already have a face, so we have already computed xprime,yprime,\n","      (x,y,w,h)=(x_prime+curr_face[0], y_prime+curr_face[1], curr_face[2], curr_face[3]) #updating with the new coordinates (primes)\n","    #so we update our region \n","    prev = (x,y,w,h)\n","    #we plot the rectangle \n","    cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0),2)    \n","  return curr_face"],"metadata":{"id":"1MrBVMr5ohbA","executionInfo":{"status":"ok","timestamp":1648826254113,"user_tz":-120,"elapsed":42,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LedelU-B12EY"},"source":["## TASK 3\n","The goal of this task is to build a histogram corresponding to the face and then track it. So the first step is to detect the face frame, compute the histogram and track the face using cam shift. This is done in the following cells:"]},{"cell_type":"markdown","source":["#### 1. Face detection\n","In this part of the task, the face frame is detected using the algorithm implemented in task 2 ( corrected version) in order to calculate the histogram."],"metadata":{"id":"A9sCXC4OAEPD"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"BbGTsrMh5HfP","colab":{"base_uri":"https://localhost:8080/","height":729},"executionInfo":{"status":"ok","timestamp":1648826280419,"user_tz":-120,"elapsed":26347,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}},"outputId":"12446c76-4053-45f4-c0b4-abc4f37a11cb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div'); //create new div element\n","      document.body.appendChild(div); //add the content of the new element to the DOM\n","\n","      video = document.createElement('video'); //create new video element\n","      video.setAttribute('playsinline', ''); //setting attributes of the element\n","\n","      div.appendChild(video); //add the content of video the the div element\n","\n","      //Selecting facing mode of the video stream\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play(); //playing video\n","\n","      canvas =  document.createElement('canvas'); //create new canvas element\n","      // set canvas size \n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div'); //create a new div element, will contains the output\n","      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n","      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n","      div_out.appendChild(img); //add the image element to the output div\n","    }\n","\n","    //taking the capture and storing it\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n","            result = canvas.toDataURL('image/jpeg', 0.8);\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    //displaying the capture \n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "]},"metadata":{}}],"source":["start_time = time.time()\n","VideoCapture()\n","eval_js('create()')\n","\n","#out first starting box: \n","prev = None \n","margin = 50 #randomly chosen\n","b = True\n","c = 0 #counter just to stop the algorithm, since it takes time (infinit loop)\n","while b:\n","  byte = eval_js('capture()')\n","  im = byte2image(byte) \n","  curr_face = detect_face(im,prev,margin)\n","   \n","  if c>20:\n","    #in this part, I decided to display only the face frame so I can play with the distance to the camera and I didn't display the big rectangle\n","    #around, if you want to display it, you just have to return \"prev\" from the fuction that computes the face detection and display it using:\n","    #eval_js('showimg(\"{}\")'.format(image2byte(im[prev[1]:prev[1]+prev[3], prev[0]:prev[0]+prev[2]])))\n","    face_frame = im[curr_face[1]:curr_face[1]+curr_face[3], curr_face[0]:curr_face[0]+curr_face[2]]\n","    tracking_window_face = curr_face\n","    eval_js('showimg(\"{}\")'.format(image2byte(face_frame)))\n","    break\n","  c +=1\n","  eval_js('showimg(\"{}\")'.format(image2byte(im)))"]},{"cell_type":"markdown","source":["#### 2. Computing histogram for detected face"],"metadata":{"id":"S_lGriWgAYsS"}},{"cell_type":"markdown","source":["Now that the face frame has been detected, the histogram of the face colour is computed. This is done using the HUE channel. It allows us afterwards to make a back projection. The aim is to, taking into account this histogram, detect the parts of the image that have similar colours to the face. In other words, for each pixel of the image, the probability that its colour belongs to the histogram is computed. Thus, we can capture the elements of the image that have the same colour as the face. If we forget the fact of wearing clothes or having objects of the same colour as the face, the parts of the image that will have a very high probability will be the visible parts of the body (i.e. the hands, neck, shoulders, arms..."],"metadata":{"id":"pEW4IIQ6ZpMC"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"5u9JmiHemAYh","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1648826286374,"user_tz":-120,"elapsed":296,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}},"outputId":"d3ae67eb-9295-4bd9-856f-4b4d880e266b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAA2CAYAAADEUyJ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAIDElEQVR4nO3dfYxcVR3G8e/jFloLAi1oQUosIC0iimAVlGiEAqKSlkSMGiUlQjBEXjREpZIQ4x9awYgmGk1TqkQJL6kojQGhvOkfClJRoKViERVaWsqbvAuUPv5x7+q4zrKzncucWeb5JM3cmbk559nd6W/unLnnHtkmIiJe/V5TOkBERPRGCn5ExIBIwY+IGBAp+BERAyIFPyJiQKTgR0QMiK4KvqTpklZKWlffThtlv5ck/an+t6KbPiMiYtuom/PwJZ0PPGZ7saRzgGm2v9xmv6dt79hFzoiI6FK3Bf8e4AO2N0raA7jZ9pw2+6XgR0QU1u0Y/gzbG+vtTcCMUfabImmVpFskHd9lnxERsQ0mjbWDpOuB3ds8dW7rHduWNNrHhTfZ3iBpH+BGSXfZ/mubvk4FTgUYYuidU9lpzB+gEy/O2KGRdiY919xlKPTks421FROXpkxurK39Zj/eSDurH3l9I+0ATN70XCPteOvWRtoZBE/x+CO22/4Rxyz4to8a7TlJD0nao2VIZ/MobWyob++TdDNwMPB/Bd/2EmAJwE6a7kOHjhkrXkceXHhoI+3suvrFRtoBmHzNbY21FRPX0L6zG2vr6muvaKSd/Zee1kg7AHtfsLqRdrY+0+AB0taXmmurD13v5f8Y7bluh3RWAAvr7YXAVSN3kDRN0uR6ezfgcODuLvuNiIhx6rbgLwaOlrQOOKq+j6S5kpbW+7wFWCXpDuAmYLHtFPyIiB4bc0jn5dh+FJjX5vFVwCn19m+Bt3XTT0REdC8zbSMiBkQjBV/SsZLukXRvPQFr5POTJV1eP3+rpFlN9BsREZ3ruuBLGgK+D3wIOAD4pKQDRux2MvC47TcDFwLf7LbfiIgYnyaO8N8N3Gv7PtsvAJcBC0bsswC4uN5eDsyTpAb6joiIDjVR8PcEHmi5v75+rO0+trcATwC7NtB3RER0qKuzdJrWOtN2ClMLp4mIeHVp4gh/A7BXy/2Z9WNt95E0CdgZeHRkQ7aX2J5re+52NDflPCIimin4twH7Sdpb0vbAJ6hm4LZqnZF7AnCju7lMZ0REjFvXQzq2t0g6HbgWGAKW2V4j6WvAKtsrgIuAn0i6F3iM6k0hIiJ6qJExfNtXA1ePeOy8lu1/AR9roq+IiNg2vZp4dZKkh1uWOTyliX4jIqJzXR/ht0y8OprqlMzbJK1oc4G0y22f3m1/ERGxbXo18SoiIgrr1cQrgI9KulPSckl7tXk+IiJeQV0tYg4g6QTgWNun1PdPBA5tHb6RtCvwtO3nJX0W+LjtI9u09Z+JV8Ac4J4OIuwGPNLVD9G8ZOpMP2aC/syVTJ1JpmpJ2bZLHDZR8N8DfNX2B+v7iwBsf2OU/YeAx2zv3FXH/21vle25TbTVlGTqTD9mgv7MlUydSaaX15OJV/V6t8PmA2sb6DciIsahVxOvzpQ0H9hCNfHqpG77jYiI8enVxKtFwKIm+mpjySvUbjeSqTP9mAn6M1cydSaZXkbXY/gRETExZE3biIgBMWEL/liXcyhB0l6SbpJ0t6Q1ks4qnQmqM6Mk/VHSL0tnGSZpl3pOxp8lra3P9iqd6Qv13221pEslTSmUY5mkzZJWtzw2XdJKSevq22l9kOmC+u93p6SfS9qldKaW586WZEm79UMmSWfUv6s1ks7vZaZWE7Lgd7iObglbgLNtHwAcBnyuT3KdRf+dGfVd4Fe29wcOonA+SXsCZwJzbR9IdQJCqau6/hg4dsRj5wA32N4PuKG+XzrTSuBA228H/sIr9z3deDJRT+w8Bri/x3mgTSZJR1BdfeAg228FvlUgFzBBCz59ejkH2xtt315vP0VVxNrNOu4ZSTOBjwBLS+ZoJWln4P1Ul83G9gu2/1k2FVCdxPDaepGeqcCDJULY/g3V2WytWteFvhg4vnQm29fVS5YC3EK1+FHRTLULgS8BPf+CcpRMpwGLbT9f77O517mGTdSC3+nlHIqRNAs4GLi1bBK+Q/Xi31o4R6u9gYeBH9VDTUsl7VAykO0NVEde9wMbgSdsX1cy0wgzbG+stzcBM0qGaeMzwDWlQ0haAGywfUfpLC1mA++TdKukX0t6V6kgE7Xg9zVJOwI/Az5v+8mCOY4DNtv+Q6kMo5gEHAL8wPbBwDP0fojif9Rj4guo3ozeCOwg6dMlM42mXi2ub06vk3Qu1XDmJYVzTAW+Apw31r49NgmYTjXM+0XgCkkqEWSiFvxO1tEtQtJ2VMX+EttXFo5zODBf0t+phr2OlPTTspGA6hPZetvDn36WU70BlHQU8DfbD9t+EbgSeG/hTK0eGp6xXt8WGxZoJekk4DjgU32wbOm+VG/Yd9Sv+ZnA7ZJ2L5qqer1f6crvqT5t9/TL5GETteB3so5uz9Xv2hcBa21/u3Qe24tsz7Q9i+p3dKPt4kettjcBD0iaUz80Dxi5fkKv3Q8cJmlq/XecR3990d26LvRC4KqCWYDqTDmq4cL5tp8tncf2XbbfYHtW/ZpfDxxSv95K+gVwBICk2cD2FLrA24Qs+PUXRcOXc1gLXGF7TdlUQHVEfSLVkfTw6l4fLh2qT50BXCLpTuAdwNdLhqk/bSwHbgfuovq/UWSGpKRLgd8BcyStl3QysBg4WtI6qk8ji/sg0/eA1wEr69f6D/sgU1GjZFoG7FOfqnkZsLDUp6HMtI2IGBAT8gg/IiLGLwU/ImJApOBHRAyIFPyIiAGRgh8RMSBS8CMiBkQKfkTEgEjBj4gYEP8GUyTtyMsUmBYAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}}],"source":["def hsv(face_frame):\n","  \"\"\"\n","  Transforms the face frame into HSV and computes histogram\n","  @params:\n","        - face_frame: the frame representing the face\n","  @return:\n","        - mask: hsv with mask to deal with brightness and darkness pixels\n","        - histo: histogram computed from the hsv\n","  \"\"\"\n","  #transforming the detected face frame into HSV\n","  hsv = cv2.cvtColor(face_frame, cv2.COLOR_BGR2HSV)\n","  #Creating a mask using inRange for the pixels to deal with brightness and darkness\n","  #allows us to take into consideration the pixels that are too dark and/or too bright\n","  #the parameters of the mask were found just by playing and testing many of values (the tests were held in one only place)\n","  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n","  #computing histogram of face frame using hue channel ie [0]\n","  #here we used: 18 as bins for the histogram\n","  #and the range of the hue was set to [0,180]\n","  histo = cv2.calcHist([hsv],[0], mask, [18], [0,180])\n","  #normalizing histogram 0-255\n","  histo = cv2.normalize(histo, histo, 0, 255, cv2.NORM_MINMAX)\n","  return mask, hsv,histo\n","\n","mask,hsv,histo = hsv(face_frame)\n","#Displaying histogram\n","plt.imshow(histo.reshape(1,-1))\n","plt.show()"]},{"cell_type":"code","source":["#more meaningful/ clearer display\n","plt.hist(histo)"],"metadata":{"id":"HMk-9fKL1yXn","colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"status":"ok","timestamp":1648826288268,"user_tz":-120,"elapsed":280,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}},"outputId":"8755ffbe-8add-42bf-f97f-b5f326cf0fbe"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([13.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.]),\n"," array([  0. ,  25.5,  51. ,  76.5, 102. , 127.5, 153. , 178.5, 204. ,\n","        229.5, 255. ], dtype=float32),\n"," <a list of 10 Patch objects>)"]},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["#### 3. CAMshift application"],"metadata":{"id":"LicYjXVeBgGf"}},{"cell_type":"markdown","source":["Now, the aim is to track the face on the image (the capture taken via the camera) using the back projection of the histogram calculated previously. To do this, we use CAMShift (Continuously Adaptive Meanshift). It is an algorithm that allows us to track the face by adapting to different constraints we may have. Constraints such as having a tilted face, a face far or close to the camera, etc."],"metadata":{"id":"PPJHr7FCbB3P"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"yosuDFMZy2hK","colab":{"base_uri":"https://localhost:8080/","height":985},"executionInfo":{"status":"ok","timestamp":1648826396712,"user_tz":-120,"elapsed":12534,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}},"outputId":"ac5292d0-97d5-421c-cc0a-33c1e38e08a3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div'); //create new div element\n","      document.body.appendChild(div); //add the content of the new element to the DOM\n","\n","      video = document.createElement('video'); //create new video element\n","      video.setAttribute('playsinline', ''); //setting attributes of the element\n","\n","      div.appendChild(video); //add the content of video the the div element\n","\n","      //Selecting facing mode of the video stream\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play(); //playing video\n","\n","      canvas =  document.createElement('canvas'); //create new canvas element\n","      // set canvas size \n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div'); //create a new div element, will contains the output\n","      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n","      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n","      div_out.appendChild(img); //add the image element to the output div\n","    }\n","\n","    //taking the capture and storing it\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n","            result = canvas.toDataURL('image/jpeg', 0.8);\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    //displaying the capture \n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "]},"metadata":{}}],"source":["######### Cam shift########\n","VideoCapture()\n","eval_js('create()')\n","#this following line describes a criteria to stop camshift algorithm\n","#so this algorithm stops when 10 iterations have been carried out or when the computed value is not changing in all the direction by a factor of 1pt \n","term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","c = 0 #this counter is used to stop the algorithm (because it takes time since my laptop camera is not good)\n","tracking_window_hand =  (0,0,im.shape[1],im.shape[0]) #defiens hand window used for tracking the hand\n","while True:\n","  byte = eval_js('capture()') # capture\n","  im = byte2image(byte) #converting capture \n","  # Converting the image to HSV\n","  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  # Computing mask (inRange) as done in the previous cell\n","  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n","  # Back projecting the frame histogram into the hsv image\n","  # We use only channel Hue (ie channel 0)\n","  # and the range of the hue was set to [0,180]\n","  #basically, we have the histogram of colors of the face and we will backproject it in our current image \n","  #to detect the part of the image that fit the histogram (have same color as the face)\n","  prob = cv2.calcBackProject([hsv],[0], histo, [0,180],scale= 1)\n","  \n","  # Apply the mask to the backprojection output\n","  # Helps us to deal with dark or/and bright pixels\n","  prob = prob & mask\n","\n","  # Applying Cam shift\n","  # it takes as arguments:\n","  # the computed back projection with mask\n","  # the windows where the face is stored \n","  # term_crit, the criteria that stops the algorithm \n","  _, tracking_window_face = cv2.CamShift(prob, tracking_window_face, term_crit)\n","  # plot a bounding box with coordiantes `tracking_window_face` in the image\n","  (x,y,w,h) = tracking_window_face\n","  cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0), 2)  \n","\n","  if c>10:  #this helps to conditionnally stop the algo\n","    break\n","  c +=1 \n","  #eval_js('showimg(\"{}\")'.format(image2byte(im))) #displaying image\n","  #eval_js('showimg(\"{}\")'.format(image2byte(mask))) #displaying mask\n","  eval_js('showimg(\"{}\")'.format(image2byte(prob))) #displaying back projection output"]},{"cell_type":"markdown","metadata":{"id":"sU0ANOwS1wGi"},"source":["## TASK 4"]},{"cell_type":"markdown","source":["The purpose of this task is to detect the hand. To do this we have to erase the face. To better understand, as we have the color histogram corresponding to the color of the face (computed in task 3), we can track it on the image but we have also seen that we can track the other parts of the body, in particular the hands. And here, we want to keep only the latter. So we start by detecting the face and tracking it with CAMShift and then erase it by setting its probability to 0 (the probability on the face region). Then, we use CAMshift to detect the hands that will have the highest probability (using a backprojection of the face histogram) in the image without the face."],"metadata":{"id":"o3UwlbG3c4ZR"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"ZFXko2EF1yjo","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1648826505566,"user_tz":-120,"elapsed":30188,"user":{"displayName":"Lynda Att","userId":"09343934939883857258"}},"outputId":"dfc49068-0a50-4851-dba2-80efd679a576"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div'); //create new div element\n","      document.body.appendChild(div); //add the content of the new element to the DOM\n","\n","      video = document.createElement('video'); //create new video element\n","      video.setAttribute('playsinline', ''); //setting attributes of the element\n","\n","      div.appendChild(video); //add the content of video the the div element\n","\n","      //Selecting facing mode of the video stream\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play(); //playing video\n","\n","      canvas =  document.createElement('canvas'); //create new canvas element\n","      // set canvas size \n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div'); //create a new div element, will contains the output\n","      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n","      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n","      div_out.appendChild(img); //add the image element to the output div\n","    }\n","\n","    //taking the capture and storing it\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n","            result = canvas.toDataURL('image/jpeg', 0.8);\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    //displaying the capture \n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-feae1fc6e54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["VideoCapture()\n","eval_js('create()')\n","#this following line describes a criteria to stop camshift algorithm\n","#so this algorithm stops when 10 iterations have been carried out or when the computed value is not changing in all the direction by a factor of 1pt \n","term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n","tracking_window_hand = (0,0,im.shape[1],im.shape[0]) #to keep track of the hand\n","c = 0 #couter to stop the algo\n","while True:\n","  byte = eval_js('capture()') # capture\n","  im = byte2image(byte) #converting capture \n","  # Converting the image to HSV\n","  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  # Computing mask (inRange) as done in the previous cell\n","  mask = cv2.inRange(hsv,np.array((0,64,32)), np.array((180,200,200)))\n","  # Back projecting the face frame histogram into the hsv image\n","  #basically, we have the histogram of colors of the face and we will backproject it in our current image \n","  #to detect the part of the image that fit the histogram (have same color as the face)\n","  prob = cv2.calcBackProject([hsv],[0], histo, [0,180],scale= 1)\n","  \n","  # Apply the mask to the backprojection output\n","  # Helps us to deal with dark or/and bright pixels\n","  prob = prob & mask\n","\n","  #Tracking face \n","  # Applying cam shift\n","  (x,y,w,h) = tracking_window_face\n","  ret,tracking_window_face = cv2.CamShift(prob,tracking_window_face, term_crit)\n","  # Retrieve the rotated bounding rectangle\n","  pts = cv2.boxPoints(ret).astype(np.int)\n","  # fill the face area (prob) with zeros\n","  cv2.fillPoly(prob, [pts], 0)\n","  # Draw the face area\n","  cv2.polylines(im, [pts], True, (255, 255 , 0), 2)\n"," \n","  #Tracking hand\n","  ret2, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n","  \n","  pts2 = cv2.boxPoints(ret2).astype(np.int)\n","  #drawing the rectangle around hand \n","  cv2.polylines(im, [pts2], True, (255, 0, 255 ), 2)\n","  \n","  if c>20: #allows to stop the algorithm (my laptop camera is quite bad so it takes too much time to have a precise result, so I needed to stop it)\n","    break    \n","  c+=1\n","  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n","  eval_js('showimg(\"{}\")'.format(image2byte(prob)))"]},{"cell_type":"code","source":[""],"metadata":{"id":"qVRAjy2-RO6Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lynda_attouche_Task_3_4.ipynb","provenance":[],"authorship_tag":"ABX9TyOZUlVfWtctzgRyq/jPOfRz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}