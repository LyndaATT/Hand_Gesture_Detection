{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lynda_attouche_task9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------\n",
        "***********************************************************************************************************\n",
        "\n",
        "# <span style=\"color:Purple\"> Computer vision for machine learning Project: \"Detecting hand gestures\"\n",
        "\n",
        "\n",
        "#### Task 9: \n",
        "\n",
        "#### Author: Lynda Attouche\n",
        "#### Link: https://colab.research.google.com/drive/1Wcj0j0PrfgYsSPSDUW0kEjo_NNHxTk9e?usp=sharing\n",
        "*******************************************************************************************\n",
        "----------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Z8PxT1P9mFcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README\n",
        "* Throughout this notebook, no special commands are needed to run the code. Simply run the cells in order.\n",
        "\n",
        "\n",
        "In the previous tasks, we used image frames to train our models and make predictions. In this task, we will, instead, first extract features from the images using a CNN. In this case, we use a VGG pre-trained neural network via transfer learning. Then, we will build a model using these features as input to the new model that will be implemented. "
      ],
      "metadata": {
        "id": "b0EUL4Y_mL7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "iEmFSNPhmPyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zBrYoNtU2HpW"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Dropout\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Javascript\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "from base64 import b64decode, b64encode\n",
        "import random\n",
        "\n",
        "import glob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv2tQ7664EMb",
        "outputId": "751ad949-9b6f-431f-c8c2-57e66616dd9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path1 = \"/content/drive/MyDrive/ComputerVision/HandGesture_1/\" #balanced + variaability\n",
        "path = \"/content/drive/MyDrive/ComputerVision/\""
      ],
      "metadata": {
        "id": "8nfCZdrR2NlX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating dataset: \"Features extraction\""
      ],
      "metadata": {
        "id": "baw8GBatoT1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will upload the 224x224 size images to feed the VGG19 so that it can extract their features.We will extract features of the balanced and variable dataset."
      ],
      "metadata": {
        "id": "edIEVWFbo4dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_creation(path,dataset_path,num_dataset):\n",
        "  \"\"\"\n",
        "  Writes images in txt file\n",
        "  @params:\n",
        "          path: path to the file\n",
        "          dataset_path : path to images\n",
        "  @return:\n",
        "          txt file in colab with all images written on it\n",
        "  \"\"\"\n",
        "  images_y = os.listdir(dataset_path+'y/') #reading folder containing images of label y\n",
        "  images_a = os.listdir(dataset_path+'a/') #reading folder containing images of label a\n",
        "  images_e = os.listdir(dataset_path+'e/') #reading folder containing images of label e\n",
        "  images_k = os.listdir(dataset_path+'k/') #reading folder containing images of label k\n",
        "  n = int(.5*(len(images_y)+len(images_a)+len(images_e)+len(images_k)))\n",
        "  data = np.empty((n, 224, 224, 3))\n",
        "  labels = []\n",
        "  with open(path+'dataset'+str(num_dataset)+'.txt','w') as f: #writing on the file dataset\n",
        "    i = 0\n",
        "    c = 0\n",
        "    for im_ in images_y: #for each image of label Y\n",
        "      im =  image.load_img(dataset_path+'y/'+im_,cv2.IMREAD_GRAYSCALE) #reading current image\n",
        "      im = image.img_to_array(im)\n",
        "      if im.shape[0] == 224: #selecting only images of shape 16\n",
        "        im =  preprocess_input(im)\n",
        "        data[i] = im \n",
        "        i+=1\n",
        "        labels.append('Y')\n",
        "\n",
        "    for im_ in images_a: #for each image of label A\n",
        "      im =  image.load_img(dataset_path+'a/'+im_,cv2.IMREAD_GRAYSCALE) #reading current image\n",
        "      im = image.img_to_array(im)\n",
        "      if im.shape[0] == 224: #selecting only images of shape 16\n",
        "        im =  preprocess_input(im)\n",
        "        data[i] = im \n",
        "        i+=1\n",
        "        labels.append('A')\n",
        "\n",
        "    for im_ in images_e: #for each image of label E\n",
        "      im =  image.load_img(dataset_path+'e/'+im_,cv2.IMREAD_GRAYSCALE) #reading current image\n",
        "      im = image.img_to_array(im)\n",
        "      if im.shape[0] == 224: #selecting only images of shape 16\n",
        "        im =  preprocess_input(im)\n",
        "        data[i] = im \n",
        "        i+=1\n",
        "        labels.append('E')\n",
        "\n",
        "    for im_ in images_k: #for each image of label K\n",
        "      im =  image.load_img(dataset_path+'k/'+im_,cv2.IMREAD_GRAYSCALE) #reading current image\n",
        "      im = image.img_to_array(im)\n",
        "      if im.shape[0] == 224: #selecting only images of shape 16\n",
        "        im =  preprocess_input(im)\n",
        "        data[i] = im \n",
        "        i+=1\n",
        "        labels.append('K')\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "27otV-Y42Vfo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataset\n",
        "X,y = dataset_creation(path,dataset_path1,1) #Balanced dataset with variability"
      ],
      "metadata": {
        "id": "4zVROu9b3hdN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading VGG model and weights\n",
        "vgg_model = VGG19(weights='imagenet') \n",
        "model_ext_ftr = Model(inputs=vgg_model.input, outputs=[vgg_model.get_layer(\"fc2\").output]) # Get the fc2 layer instead of the prediction layer trained for ImageNet\n",
        "model_ext_ftr .summary() # print model"
      ],
      "metadata": {
        "id": "JgndMzd_5y_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4757eead-6cdb-44a0-a111-8985795a1dac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139,570,240\n",
            "Trainable params: 139,570,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#features extraction\n",
        "output_feats = model_ext_ftr.predict(np.array(X))# Extract features of our images. Size = (nb_images, 4096) "
      ],
      "metadata": {
        "id": "TsiLdo7E55VR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of features\",output_feats.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrLYF0g57ZO",
        "outputId": "f44a3dfa-225e-4b1d-ad1b-89aff3e498d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features (672, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving data in text file"
      ],
      "metadata": {
        "id": "m-bbeeKsrEGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the features are extracted, we will save them in a text file as follows. "
      ],
      "metadata": {
        "id": "DpzWiFV1_sq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_txt(output_feats,y):\n",
        "  \"\"\"\n",
        "    Saves data in text file and shuffles them\n",
        "    @params:\n",
        "        output_feats(array): extracted features to save\n",
        "        y(array): labels corresponding to features\n",
        "    @return:\n",
        "          1 if savec\n",
        "  \"\"\"\n",
        "  with open(path+\"vgg_res.txt\", \"w+\") as f:\n",
        "    for i in range(len(y)):\n",
        "      f.write(''.join(str(y[i])+',')) #writing label of the image (letter)\n",
        "      np.savetxt(f, output_feats[i],fmt='%d',newline=\",\") #writing image on the file \n",
        "      f.write(''.join('\\n'))\n",
        "  #shuffling data\n",
        "  lines = open(path+\"vgg_res.txt\").readlines()\n",
        "  random.shuffle(lines)\n",
        "  open(path+\"vgg_res.txt\", 'w').writelines(lines)\n",
        "  return 1"
      ],
      "metadata": {
        "id": "odRPpslcwgl6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_txt(output_feats,y)"
      ],
      "metadata": {
        "id": "ECXD3qGN_Fat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4abe21-9082-41a9-a2ed-69d2c24d16e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "r0dU6wKCFMun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_file_path):\n",
        "  \"\"\"\n",
        "    Loads dataset from text file \n",
        "    @param:\n",
        "          dataset_file_path (string): dataset path\n",
        "    @return: \n",
        "          samples (array): images\n",
        "          letter (array): labels\n",
        "  \"\"\"\n",
        "  with open(dataset_file_path) as f: \n",
        "      lines = [line.rstrip().split(\",\") for line in f] #reading file line by line\n",
        "  dic = {'A':0,'E':1,'K':2,'Y':3} #helps to convert string to int labels\n",
        "  letters = [dic.get(line[0]) for line in lines] #select first column of each line, it contains the letter\n",
        "  samples = np.array([lines[i][1:len(lines[i])-1] for i in range(len(lines))],dtype=float) #select the other part of line \"image\"\n",
        "  return samples, letters"
      ],
      "metadata": {
        "id": "7DM2W7b5BY0M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples,letters = load_dataset(path+'/vgg_res.txt')\n",
        "print('Dataset size:',len(samples))\n",
        "print('Dataset shape:',samples.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwXLftPbWyev",
        "outputId": "f0307849-7b1c-4e7f-8115-31270dff4137"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 672\n",
            "Dataset shape: (672, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting dataset"
      ],
      "metadata": {
        "id": "SVurnGO2XKtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(train_ratio, samples, letters):\n",
        "  \"\"\"\n",
        "  Splits data in train and validation set\n",
        "  @params:\n",
        "      train_ratio (float): proportion of the train set\n",
        "      samples (array): images\n",
        "      letters (array): labels\n",
        "  @return: \n",
        "      x_train,y_train: images and labels of training set\n",
        "      x_val,y_val: images and labels of validation set\n",
        "  \"\"\"\n",
        "  n_train_samples = int(len(samples) * train_ratio)\n",
        "  x_train, y_train = samples[:n_train_samples], letters[:n_train_samples]\n",
        "  x_val, y_val = samples[n_train_samples:], letters[n_train_samples:]\n",
        "  return x_train,y_train,x_val,y_val"
      ],
      "metadata": {
        "id": "P6f1-6bHXIhe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting\n",
        "train_ratio = 0.7  #here 70% of the data will be the trainset and 30% the validation one\n",
        "x_train,y_train,x_val,y_val = split(train_ratio, samples, letters)"
      ],
      "metadata": {
        "id": "AxC6FyhWXOQI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "# we normalize data (convert value to range from 0 to 1)\n",
        "# this is done to reduce computation complexity (because origin values are high and thus it's more complex)\n",
        "# by this reduction, the computation will be faster and easier\n",
        "x_train /= 255\n",
        "x_val /= 255\n",
        "# convert class vectors to binary class matrices\n",
        "# converting them is required to train our models\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)"
      ],
      "metadata": {
        "id": "SWSjlWEnXa8r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating model"
      ],
      "metadata": {
        "id": "tBMPhEv5tCW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential() #sequential network\n",
        "model.add(Dense(150, activation='relu', input_shape=(4096,))) #first hidden layer with 150 neurons, it takes input of shape (4096,), which is the shape of our images in the text file\n",
        "#we use relu activation function to avoid gradient vanishing\n",
        "model.add(Dropout(0.2)) #dropout layer with proba 0.2\n",
        "model.add(Dense(50, activation='relu')) #dense layer with 50 neurons\n",
        "model.add(Dense(num_classes, activation='softmax')) #output layer: number_classes (4) and we use the activation function softmax because we have a multiclassification problem\n",
        "\n",
        "model.summary() #print a summary of the neural network\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', #we use categorical cross entrpy as loss function since we have many classes \n",
        "              optimizer='adam', #as an optimizer Adam is used to speed up computations \n",
        "              metrics=['accuracy']) #as metric we use accuracy (proportion of true results among all the predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvvftWJjXjvK",
        "outputId": "8db1bdbc-00b4-40ef-ea8f-4aa43dc932b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 150)               614550    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 150)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                7550      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 204       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 622,304\n",
            "Trainable params: 622,304\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training and evaluating"
      ],
      "metadata": {
        "id": "BDGiWhAItHX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc_loss(str,history):\n",
        "    \"\"\"\n",
        "    Plot accuracy and loss of a model\n",
        "    @params:\n",
        "            - history: history of the model\n",
        "    @return:\n",
        "            plots\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "    fig.suptitle(str, fontsize=16)\n",
        "\n",
        "    l = list(history.history.keys())\n",
        "    print(l)\n",
        "    # accuracy plot\n",
        "    ax[0].plot(history.history[l[1]])\n",
        "    ax[0].plot(history.history[l[3]])\n",
        "    ax[0].set_title('Accuracy')\n",
        "    ax[0].set_ylabel('Accuracy')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].legend(['Train', 'Valid'], loc='upper left')\n",
        "\n",
        "    # loss plot\n",
        "    ax[1].plot(history.history[l[0]])\n",
        "    ax[1].plot(history.history[l[2]])\n",
        "    ax[1].set_title('Loss') \n",
        "    ax[1].set_ylabel('Loss') \n",
        "    ax[1].set_xlabel('Epoch') \n",
        "    ax[1].legend(['Train', 'Valid'], loc='upper left') \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sMguqjC6YiJy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "#as an optimization technique, early stopping is used with respect to accuracy, it will help us to stop the training before it overfits \n",
        "callback = tf.keras.callbacks.EarlyStopping(patience=3) \n",
        "#training model on the train set\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                   callbacks=[callback]\n",
        "                    )\n",
        "#evaluating the model using validation set\n",
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Validation loss: ', score[0])\n",
        "print('Validation accuracy: ', score[1])"
      ],
      "metadata": {
        "id": "4En9Wj6gtJuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc_loss('',history) #plotting accuracy and loss function"
      ],
      "metadata": {
        "id": "xwzSdCAvX2aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results obtained are superior to those obtained in task 7. The model converges faster and reaches 90% accuracy. \n",
        "Thus, the feature extraction method shows a significant improvement.  "
      ],
      "metadata": {
        "id": "6qGgqYKVlQ0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving model and weights"
      ],
      "metadata": {
        "id": "0hQSN7dhcK1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now that the model is generated, we save it so that we can test it live as we did in task 8."
      ],
      "metadata": {
        "id": "fp641vAelStM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(path+\"/\"+\"model.json\", \"w\") as json_file:\n",
        "     json_file.write(model_json)\n",
        "model.save_weights(path+\"/\"+\"model_weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD_5TL9OcOAU",
        "outputId": "8f880c7f-ddac-41e3-8c62-7ccc8bfcf3c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    }
  ]
}
