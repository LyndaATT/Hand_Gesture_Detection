{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lynda_Attouche_Task_1_2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------\n",
        "***********************************************************************************************************\n",
        "\n",
        "# <span style=\"color:Purple\"> Computer vision for machine learning Project: \"Detecting hand gestures\"\n",
        "\n",
        "\n",
        "#### Task 1 & 2: \n",
        "\n",
        "#### Author: Lynda Attouche\n",
        "#### Link: https://colab.research.google.com/drive/15_jQRmQlB2_xS-lNTKlxIHEmjEfgAeFD?usp=sharing\n",
        "***********************************************************************************************************\n",
        "----------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "J5BJeJdOfBbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README\n",
        "* Throughout this notebook, no special commands are needed to run the code. Simply run the cells in order. \n",
        "\n",
        "* The code of the first task loops endlessly. In order to move on to the next task, just stop it manually and run from the next cell "
      ],
      "metadata": {
        "id": "tC6wbizDcWQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "82H2y1dWawlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Libraries"
      ],
      "metadata": {
        "id": "OdSmC5b7a4wx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFvfnWFtvn7h"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import cv2\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenCV"
      ],
      "metadata": {
        "id": "UBso4aW8a_kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/opencv/opencv/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJU-LZCDxfpz",
        "outputId": "4eee8ebb-1f2f-4a54-81b8-25999885774f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'opencv'...\n",
            "remote: Enumerating objects: 303962, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 303962 (delta 0), reused 1 (delta 0), pack-reused 303960\u001b[K\n",
            "Receiving objects: 100% (303962/303962), 493.11 MiB | 24.71 MiB/s, done.\n",
            "Resolving deltas: 100% (211516/211516), done.\n",
            "Checking out files: 100% (7028/7028), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful functions"
      ],
      "metadata": {
        "id": "SGqM4SF1XtSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a real time video stream\n",
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)"
      ],
      "metadata": {
        "id": "fu0FhxPsvr4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting image types\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "metadata": {
        "id": "eQDEdW8Uv1ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Cascade classifiers from git repository\n"
      ],
      "metadata": {
        "id": "W9WkwnqnYP5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade_path = \"/content/opencv/data/haarcascades/haarcascade_frontalface_alt.xml\"\n",
        "eyes_cascade_path = \"/content/opencv/data/haarcascades/haarcascade_eye.xml\"\n",
        "face_cascades = cv2.CascadeClassifier(face_cascade_path)\n",
        "eyes_cascades = cv2.CascadeClassifier(eyes_cascade_path)"
      ],
      "metadata": {
        "id": "PrhGQnP83kRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faces and eyes detection"
      ],
      "metadata": {
        "id": "yJzinwFdYuJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Basic face and eyes Detection"
      ],
      "metadata": {
        "id": "Nj65Zzf2Ya2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this first task is to detect the faces and eyes that can be included in an image, in this case a real time capture from our computer camera. \n",
        "\n",
        "The detection is done thanks to **opencv**'s face and eye cascades classifiers. They are used in the following cell. \n",
        "\n",
        "This function starts with the detection of faces on the whole image. To illustrate this, rectangles are drawn around the faces using the positions of the faces returned by the **face_cascade.detectMultiScale** classifier. Next, now that the face region is available, the eyes need to be detected. To do this, the **eyes_cascades.detectMultiScale** classifier is called and provided with the face region as an argument. \n",
        "Similarly, rectangles are drawn around the eyes to show the detection made. "
      ],
      "metadata": {
        "id": "VssDjQWth7Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def faceyesDetection(im):\n",
        "  \"\"\"\n",
        "      Detects faces and eyes on image\n",
        "      @params:\n",
        "              - im (array): image on which the detection will take place\n",
        "      @return:\n",
        "              - plotting rectangles showing the detection of eyes and faces on the image/capture\n",
        "  \"\"\"\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY) #take the gray scale version\n",
        "  \n",
        "  faces = face_cascades.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=4)   #face detection using face cascade \n",
        "  for (x, y, w, h) in faces: #here x,y point coord top left corner, w and h are width and height of the capture\n",
        "    cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0),2) #(x+w,y+h) is bottom right corner\n",
        "    face_region_gray = gray[y:y+h, x:x+w] # face region detected in gray scale\n",
        "    face_region = im[y:y+h, x:x+w] # face region  detected\n",
        "\n",
        "    eyes = eyes_cascades.detectMultiScale(face_region_gray, 1.1, 3) # Eyes detection in face region \n",
        "    for (xeye,yeye,weye,heye) in eyes: #here xeye,yeye point coord top left corner of face region, weye and heye are width and height of the face region\n",
        "      cv2.rectangle( face_region, (xeye,yeye), (xeye+weye,yeye+heye), (0,255,0),2) # Draw rectangle on detected eyes\n"
      ],
      "metadata": {
        "id": "VBo8qmbN6Dvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application of the function above on image (live stream video capture)"
      ],
      "metadata": {
        "id": "23ShMOgzkI6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "x = True\n",
        "while x:\n",
        "  #Capturing live stream video\n",
        "  byte =  eval_js('capture()') \n",
        "  #Converting the capture to image type\n",
        "  im = byte2image(byte) \n",
        "  #Detecting faces and eyes on the capture (detection+drawing)\n",
        "  faceyesDetection(im)\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "  print(\"%s s\" % (time.time() - start_time))\n",
        "  r = input(\"stop?\")\n",
        "  if r=='y':\n",
        "    break"
      ],
      "metadata": {
        "id": "y5tB27bUyHFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comments & Conclusion:**\n",
        "\n",
        "Several Tests were carried out with the previous code, **i.e.** detection of: one person, several people at the same time, person in a photo, person with objects close to the face (necklace in this case), person in slight movement and finally with different brightnesses. \n",
        "\n",
        "From the results obtained through these tests, the cascade classifiers used generally detect faces **quite well**. \n",
        "\n",
        "However, they are still quite **limited**. Indeed, for example, when there is too much light or in a more or less dark environment, detection is almost impossible. In addition, when there is slight movement of the face, the detection in addition to taking time, does not really result in a clean detection. And finally, when objects are close to the face, the algorithm tended to detect them as well. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4A80VQ4ckTFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Reducing the computation time of Facedetect"
      ],
      "metadata": {
        "id": "a8XgLvUkaJ3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted earlier, the classical algorithm used in task 1 can take a fair number of seconds to do the detection. This is due to the fact that the classifier scans/searches through the whole image. Thus, in this task, the computation will only be done from the region of interest and this will reduce the computation time. "
      ],
      "metadata": {
        "id": "Va2PfEqgoB0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, a function has been defined in order to calculate the region on which the face search will be carried out at time **t+1** taking into account the previous face delimitation at time **t** (if we refer to the figure on moodle)"
      ],
      "metadata": {
        "id": "Yr426vLGpotk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_region(img,margin,prev):\n",
        "  \"\"\"\n",
        "  Computes  region of interest \n",
        "  @params:\n",
        "          - img (array): image on which detection will be done\n",
        "          - margin (int): the margin to be taken from the previous region (the coordinate shift)\n",
        "          - prev (array): previous face detection\n",
        "  @return \n",
        "          - region of interest \n",
        "  \"\"\"\n",
        "  # as seen in the previous task, the cascadeClassifier returns a face as a rectangle\n",
        "  # so this is the case for the param prev, ie: prev = (x,y,w,h) = (prev[0],prev[1],prev[2],prev[3])\n",
        "  # where (x,y) is the top left corner and (w,h) the bottom right corner\n",
        "  # the goal is then to compute (x',y',w',h') considering the margin and previous region to define the new region such that:\n",
        "  # x' = x - margin\n",
        "  # y' = y - margin \n",
        "  # w' = (x+w) + margin\n",
        "  # h' = (y+h) + margin\n",
        "\n",
        "  #top left corner\n",
        "  x_prime = prev[0] - margin \n",
        "  y_prime = prev[1] - margin \n",
        "\n",
        "  #bottom right\n",
        "  w_prime = prev[0]+prev[2]+margin \n",
        "  h_prime = prev[1]+prev[3]+margin \n",
        "\n",
        "  # Note: \n",
        "  #the new region must stay in the image and not be out of it (I noticed that after some tests because the results were weird)\n",
        "  # i.e:\n",
        "  #top left corner should not be negative (as a subtraction is made from the previous x and y) càd:  x_prime>=0 and y_prime>=0\n",
        "  #if either x or y (or both of them) is negative, it should be set to 0\n",
        "  x_prime= max(0,x_prime) # = 0 if x_prime<0\n",
        "  y_prime = max(0,y_prime) #= 0 if y_prime<0\n",
        "\n",
        "  #for the bottom right corner should not be out of the image in the sense that we should not obtain values that go beyond the coordinate of the \n",
        "  #image (since w' for example is a result of increasing x+w with a margin )\n",
        "  #to handle that possible problem, the value should be set to the image height or width (depends on which coordinate) of the image, as follows\n",
        "  (imgH,imgW) = im.shape[0], im.shape[1]\n",
        "  w_prime = min(imgW,w_prime) # = image width if x_prime>image width\n",
        "  h_prime = min(imgH,h_prime) # = image heigh if y_prime>image heigh\n",
        "\n",
        "  return (x_prime,y_prime,w_prime-x_prime,h_prime-y_prime)"
      ],
      "metadata": {
        "id": "QPi0Ejr3hKMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "#out first starting box: \n",
        "prev = None \n",
        "margin = 80 #randomly chosen\n",
        "b = True\n",
        "\n",
        "while b:\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte) \n",
        "  new_reg = im.copy() #starter region, whole image (first step, step t before detection)\n",
        "  #if we didn't detect a face yet\n",
        "  if prev is not None :\n",
        "    #print(\"I am here\")\n",
        "    x_prime,y_prime,w_prime,h_prime = compute_region(im,margin,prev) #computing the new region (of timestep t+1)\n",
        "    #my new region:\n",
        "    new_reg = im[y_prime:y_prime+h_prime,x_prime:x_prime+w_prime] \n",
        "    #plot the rectangle \n",
        "    cv2.rectangle(im, (x_prime,y_prime), (x_prime+w_prime, y_prime+h_prime), (255,0,0),2)\n",
        "\n",
        "  gray = cv2.cvtColor(new_reg, cv2.COLOR_BGR2GRAY) # Converting image to gray scale\n",
        "  #face detection using face cascade \n",
        "  faces = face_cascades.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=4)\n",
        "  if len(faces)==0: # no face has been detected \n",
        "    prev = None  #we won't have a roi \n",
        "  else:\n",
        "    curr_face = faces[0] #we pick the first face detected\n",
        "    if prev is None: #previous region surrounding the face picked\n",
        "      (x,y,w,h) = curr_face #get the face region coordinates\n",
        "    else: #we already have a face, so we have already computed xprime,yprime,\n",
        "      (x,y,w,h)=(x_prime+curr_face[0], y_prime+curr_face[1], curr_face[2], curr_face[3]) #updating with the new coordinates (primes)\n",
        "    #so we update our region \n",
        "    prev = (x,y,w,h)\n",
        "    #we plot the rectangle \n",
        "    cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0),2)    \n",
        "\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "  #print(\"%s s\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GkT3vSx6k1rM",
        "outputId": "df04c2b2-36b5-44fa-dca4-dbeaf422f0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a3389d639ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'showimg(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0;31m#print(\"%s s\" % (time.time() - start_time))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Following the tests made, this new method proved to be faster (which was the goal by the way). For example for one of the tests, the classical method did the computation in about 4s while this new method did it in 1s. \n",
        "So in terms of time, this new method is quite satisfactory but still has a low accuracy (at least in my case). "
      ],
      "metadata": {
        "id": "M2hXGEaa6Rvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "imRG-U9b6XWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
