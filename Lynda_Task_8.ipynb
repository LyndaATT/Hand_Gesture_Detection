{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lynda_attouche_task8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------------\n",
        "***********************************************************************************************************\n",
        "\n",
        "# <span style=\"color:Purple\"> Computer vision for machine learning Project: \"Detecting hand gestures\"\n",
        "\n",
        "\n",
        "#### Task 8: \n",
        "\n",
        "#### Author: Lynda Attouche\n",
        "#### Link: https://colab.research.google.com/drive/1QJxNdlKoWiqh0i70uOAmZ9XLsCyHePBG?usp=sharing\n",
        "*******************************************************************************************\n",
        "----------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "8BPxE7f_a38n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README\n",
        "* Throughout this notebook, no special commands are needed to run the code. Simply run the cells in order. \n",
        "\n",
        "* The code of some tasks loops endlessly, to stop them a counter has been set up. It is possible to comment the counter and so to move on to the next task, just stop it manually and run from the next cell "
      ],
      "metadata": {
        "id": "z_UWRT_CeCzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous task, we tested the models with the generated datasets. In this task, we will test our models \"live\", by directly detecting the hand in video capture, transforming it into an image and making the prediction. The different steps will be those seen in the previous, mostly tasks 5-6 and will be concluded by the prediction on the models of task 7. "
      ],
      "metadata": {
        "id": "sjCGQ1rjd9Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "Sod9JTUpbFhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Dropout\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Javascript\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "from base64 import b64decode, b64encode\n",
        "import random"
      ],
      "metadata": {
        "id": "tVRCcEycqk64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/ComputerVision/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF_T3MaGqrih",
        "outputId": "6cbdbd3f-e7fb-441a-84a0-7ae5ef9be336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/opencv/opencv/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN3J0_o_q9nA",
        "outputId": "b2af16a2-e83d-4bef-9c93-7adca9dd6600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'opencv'...\n",
            "remote: Enumerating objects: 305819, done.\u001b[K\n",
            "remote: Counting objects: 100% (267/267), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 305819 (delta 98), reused 170 (delta 64), pack-reused 305552\u001b[K\n",
            "Receiving objects: 100% (305819/305819), 495.25 MiB | 27.54 MiB/s, done.\n",
            "Resolving deltas: 100% (212738/212738), done.\n",
            "Checking out files: 100% (7048/7048), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade_path = \"/content/opencv/data/haarcascades/haarcascade_frontalface_alt.xml\"\n",
        "face_cascades = cv2.CascadeClassifier(face_cascade_path)"
      ],
      "metadata": {
        "id": "A0nN1FKdrAXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Used functions "
      ],
      "metadata": {
        "id": "-VGjF1IYrEOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting image types\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "metadata": {
        "id": "ZkC53oV_rImY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div'); //create new div element\n",
        "      document.body.appendChild(div); //add the content of the new element to the DOM\n",
        "\n",
        "      video = document.createElement('video'); //create new video element\n",
        "      video.setAttribute('playsinline', ''); //setting attributes of the element\n",
        "\n",
        "      div.appendChild(video); //add the content of video the the div element\n",
        "\n",
        "      //Selecting facing mode of the video stream\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play(); //playing video\n",
        "\n",
        "      canvas =  document.createElement('canvas'); //create new canvas element\n",
        "      // set canvas size \n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div'); //create a new div element, will contains the output\n",
        "      document.body.appendChild(div_out); //add the content of the div_out to the DOM\n",
        "      img = document.createElement('img'); //create the image element (will contain the image/capture we'll take)\n",
        "      div_out.appendChild(img); //add the image element to the output div\n",
        "    }\n",
        "\n",
        "    //taking the capture and storing it\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){ // Wait for Capture to be clicked.\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0); //draw an image onto the canvas.\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    //displaying the capture \n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)"
      ],
      "metadata": {
        "id": "9CD-T5GJrVfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_region(img,margin,prev):\n",
        "  \"\"\"\n",
        "  Computes  region of interest \n",
        "  @params:\n",
        "          - img (array): image on which detection will be done\n",
        "          - margin (int): the margin to be taken from the previous region (the coordinate shift)\n",
        "          - prev (array): previous face detection\n",
        "  @return \n",
        "          - region of interest \n",
        "  \"\"\"\n",
        "  # as seen in the previous task, the cascadeClassifier returns a face as a rectangle\n",
        "  # so this is the case for the param prev, ie: prev = (x,y,w,h) = (prev[0],prev[1],prev[2],prev[3])\n",
        "  # where (x,y) is the top left corner and (w,h) the bottom right corner\n",
        "  # the goal is then to compute (x',y',w',h') considering the margin and previous region to define the new region such that:\n",
        "  # x' = x - margin\n",
        "  # y' = y - margin \n",
        "  # w' = (x+w) + margin\n",
        "  # h' = (y+h) + margin\n",
        "\n",
        "  #top left corner\n",
        "  x_prime = prev[0] - margin \n",
        "  y_prime = prev[1] - margin \n",
        "\n",
        "  #bottom right\n",
        "  w_prime = prev[0]+prev[2]+margin \n",
        "  h_prime = prev[1]+prev[3]+margin \n",
        "\n",
        "  # Note: \n",
        "  #the new region must stay in the image and not be out of it (I noticed that after some tests because the results were weird)\n",
        "  # i.e:\n",
        "  #top left corner should not be negative (as a subtraction is made from the previous x and y) cÃ d:  x_prime>=0 and y_prime>=0\n",
        "  #if either x or y (or both of them) is negative, it should be set to 0\n",
        "  x_prime= max(0,x_prime) # = 0 if x_prime<0\n",
        "  y_prime = max(0,y_prime) #= 0 if y_prime<0\n",
        "\n",
        "  #for the bottom right corner should not be out of the image in the sense that we should not obtain values that go beyond the coordinate of the \n",
        "  #image (since w' for example is a result of increasing x+w with a margin )\n",
        "  #to handle that possible problem, the value should be set to the image height or width (depends on which coordinate) of the image, as follows\n",
        "  (imgH,imgW) = img.shape[0], img.shape[1]\n",
        "  w_prime = min(imgW,w_prime) # = image width if x_prime>image width\n",
        "  h_prime = min(imgH,h_prime) # = image heigh if y_prime>image heigh\n",
        "\n",
        "  return (x_prime,y_prime,w_prime-x_prime,h_prime-y_prime)"
      ],
      "metadata": {
        "id": "G75jmdi7rZit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face(im,prev,margin):\n",
        "  \"\"\"\n",
        "  Detects face regions\n",
        "  @param:\n",
        "        - im (array): image/capture\n",
        "        - prev (array): previous detected area\n",
        "        - margin (int): margin for detection \n",
        "  @return \n",
        "        detected region w/out margin \n",
        "  \"\"\"\n",
        "  new_reg = im.copy() #starter region, whole image (first step, step t before detection)\n",
        "  curr_face = None #contains the detected face\n",
        "  #if we didn't detect a face yet\n",
        "  if prev is not None :\n",
        "    #print(\"I am here\")\n",
        "    x_prime,y_prime,w_prime,h_prime = compute_region(im,margin,prev) #computing the new region (of timestep t+1)\n",
        "    #my new region:\n",
        "    new_reg = im[y_prime:y_prime+h_prime,x_prime:x_prime+w_prime] \n",
        "    #plot the rectangle \n",
        "    cv2.rectangle(im, (x_prime,y_prime), (x_prime+w_prime, y_prime+h_prime), (255,0,0),2)\n",
        "\n",
        "  gray = cv2.cvtColor(new_reg, cv2.COLOR_BGR2GRAY) # Converting image to gray scale\n",
        "  #face detection using face cascade \n",
        "  faces = face_cascades.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=4)\n",
        "  if len(faces)==0: # no face has been detected \n",
        "    prev = None  #we won't have a roi *\n",
        "  else:\n",
        "    curr_face = faces[0] #we pick the first face detected\n",
        "    if prev is None: #previous region surrounding the face picked\n",
        "      (x,y,w,h) = curr_face #get the face region coordinates\n",
        "    else: #we already have a face, so we have already computed xprime,yprime,\n",
        "      (x,y,w,h)=(x_prime+curr_face[0], y_prime+curr_face[1], curr_face[2], curr_face[3]) #updating with the new coordinates (primes)\n",
        "    #so we update our region \n",
        "    prev = (x,y,w,h)\n",
        "    #we plot the rectangle \n",
        "    cv2.rectangle(im, (x,y), (x+w, y+h), (255,0,0),2)    \n",
        "  return curr_face"
      ],
      "metadata": {
        "id": "3zcVS7XjrZl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hand_position(im,pts):\n",
        "  \"\"\"\n",
        "  Computing the position of the box around the hand\n",
        "  @params:\n",
        "          pts: the box points (points in corners)\n",
        "  @return:\n",
        "          position of the hand\n",
        "  \"\"\"\n",
        "  #in this following lines of code, we'll be take either the min or max so we can control the position of hand\n",
        "  #insuring that the hands stays insides the image\n",
        "  x_top_l = max(0, min(pts[:,0]))\n",
        "  y_top_l = max(0, min(pts[:,1]))\n",
        "  x_bottom_r = min(im.shape[1], max(pts[:,0]))\n",
        "  y_bottom_r = min(im.shape[0],  max(pts[:,1 ]))\n",
        "  hand =  (x_top_l, y_top_l), (x_bottom_r, y_bottom_r)\n",
        "  return hand"
      ],
      "metadata": {
        "id": "-XPFVWEPrZoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(name,weights):\n",
        "  \"\"\"\n",
        "  Loads neural network model\n",
        "  @params:\n",
        "          name(string): name of savec model\n",
        "          weights(string): name of saved weights\n",
        "  @return:\n",
        "          model\n",
        "  \"\"\"\n",
        "  # load json and create model\n",
        "  json_file = open(path+name, 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  loaded_model.load_weights(path+weights)\n",
        "  return loaded_model"
      ],
      "metadata": {
        "id": "YuL2blXJbSyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_letter(i):\n",
        "  \"\"\"\n",
        "  Converts integer to letter according to the used classes\n",
        "  @params:\n",
        "          i (int): the one we want to convert\n",
        "  @return:\n",
        "          letter corresponding to the given integer\n",
        "  \"\"\"\n",
        "  if i == 0:\n",
        "    return 'A'\n",
        "  elif i == 1:\n",
        "    return 'E'\n",
        "  elif i==2:\n",
        "    return 'K'\n",
        "  else: \n",
        "    return 'Y'"
      ],
      "metadata": {
        "id": "6aq0AyMGbU15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(hand_image,loaded_model):\n",
        "  \"\"\"\n",
        "    Makes prediction on given image of hand\n",
        "    @params:\n",
        "            hand_image(array): image of hand that we want to predict label\n",
        "            loaded_model: model to use for predicting\n",
        "    @return: \n",
        "            label of hand_image (letter format)\n",
        "  \"\"\"\n",
        "  prediction = loaded_model1.predict(hand_image) # where hand_image is the probability image of your hand of size (1,256)\n",
        "  prediction = prediction.argmax()\n",
        "  return int_to_letter(prediction)"
      ],
      "metadata": {
        "id": "LL6aA5CibYEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hand detection\n",
        "\n",
        "The main step of this task is hand detection. In order to have our data, mainly, the image of hand we want to make prediction on, we need to detect. To do so, we follow the different step seen in tasks 4,5,6: face detection, computing histogram, backprojection and capture exact hand rectangle."
      ],
      "metadata": {
        "id": "wGW_ZQmdcrHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Face detection"
      ],
      "metadata": {
        "id": "u-rDwOZTrtk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "#out first starting box: \n",
        "prev = None \n",
        "margin = 50 #randomly chosen\n",
        "b = True\n",
        "c = 0 #counter just to stop the algorithm, since it takes time (infinit loop)\n",
        "while b:\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte) \n",
        "  curr_face = detect_face(im,prev,margin)\n",
        "   \n",
        "  if c>20:\n",
        "    #in this part, I decided to display only the face frame so I can play with the distance to the camera and I didn't display the big rectangle\n",
        "    #around, if you want to display it, you just have to return \"prev\" from the fuction that computes the face detection and display it using:\n",
        "    #eval_js('showimg(\"{}\")'.format(image2byte(im[prev[1]:prev[1]+prev[3], prev[0]:prev[0]+prev[2]])))\n",
        "    face_frame = im[curr_face[1]:curr_face[1]+curr_face[3], curr_face[0]:curr_face[0]+curr_face[2]]\n",
        "    tracking_window_face = curr_face\n",
        "    eval_js('showimg(\"{}\")'.format(image2byte(face_frame)))\n",
        "    break\n",
        "  c +=1\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(im)))"
      ],
      "metadata": {
        "id": "HGx6fNbTrZrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Histogram computation"
      ],
      "metadata": {
        "id": "RCzq4F2frwpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hsv(face_frame):\n",
        "  \"\"\"\n",
        "  Transforms the face frame into HSV and computes histogram\n",
        "  @params:\n",
        "        - face_frame: the frame representing the face\n",
        "  @return:\n",
        "        - mask: hsv with mask to deal with brightness and darkness pixels\n",
        "        - histo: histogram computed from the hsv\n",
        "  \"\"\"\n",
        "  #transforming the detected face frame into HSV\n",
        "  hsv = cv2.cvtColor(face_frame, cv2.COLOR_BGR2HSV)\n",
        "  #Creating a mask using inRange for the pixels to deal with brightness and darkness\n",
        "  #allows us to take into consideration the pixels that are too dark and/or too bright\n",
        "  #the parameters of the mask were found just by playing and testing many of values (the tests were held in one only place)\n",
        "  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n",
        "  #computing histogram of face frame using hue channel ie [0]\n",
        "  #here we used: 18 as bins for the histogram\n",
        "  #and the range of the hue was set to [0,180]\n",
        "  histo = cv2.calcHist([hsv],[0], mask, [18], [0,180])\n",
        "  #normalizing histogram 0-255\n",
        "  histo = cv2.normalize(histo, histo, 0, 255, cv2.NORM_MINMAX)\n",
        "  return mask, hsv,histo\n",
        "\n",
        "mask,hsv,histo = hsv(face_frame)\n",
        "#Displaying histogram\n",
        "plt.imshow(histo.reshape(1,-1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mJl7AU46r0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Hand box detection"
      ],
      "metadata": {
        "id": "BkQ7hie1c0hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "#this following line describes a criteria to stop camshift algorithm\n",
        "#so this algorithm stops when 10 iterations have been carried out or when the computed value is not changing in all the direction by a factor of 1pt \n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "tracking_window_hand = (0,0,im.shape[1],im.shape[0]) #to keep track of the hand\n",
        "c = 0 #couter to stop the algo\n",
        "while True:\n",
        "  time.sleep(2)\n",
        "  byte = eval_js('capture()') # capture\n",
        "  im = byte2image(byte) #converting capture \n",
        "  # Converting the image to HSV\n",
        "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "  # Computing mask (inRange) as done in the previous cell\n",
        "  mask = cv2.inRange(hsv,np.array((0,60,32)), np.array((180,200,200)))\n",
        "  # Back projecting the face frame histogram into the hsv image\n",
        "  #basically, we have the histogram of colors of the face and we will backproject it in our current image \n",
        "  #to detect the part of the image that fit the histogram (have same color as the face)\n",
        "  prob = cv2.calcBackProject([hsv],[0], histo, [0,180],scale= 1)\n",
        "  \n",
        "  # Apply the mask to the backprojection output\n",
        "  # Helps us to deal with dark or/and bright pixels\n",
        "  prob = prob & mask\n",
        "\n",
        "  #Tracking face \n",
        "  # Applying cam shift\n",
        "  (x,y,w,h) = tracking_window_face\n",
        "  ret,tracking_window_face = cv2.CamShift(prob,tracking_window_face, term_crit)\n",
        "  # Retrieve the rotated bounding rectangle\n",
        "  pts = cv2.boxPoints(ret).astype(np.int)\n",
        "  # fill the face area (prob) with zeros\n",
        "  cv2.fillPoly(prob, [pts], 0)\n",
        "  # Draw the face area\n",
        "  cv2.polylines(im, [pts], True, (255, 255 , 0), 2)\n",
        " \n",
        "  #Tracking hand\n",
        "  ret2, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n",
        "  \n",
        "  pts2 = cv2.boxPoints(ret2).astype(np.int)\n",
        "  hand_image = hand_position(im,pts2)\n",
        "\n",
        "  #drawing the rectangle around hand \n",
        "  cv2.rectangle(im, hand_image[0], hand_image[1], (0,0,255), 2)\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(im)))\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(prob)))\n",
        "  c+=1\n",
        "  if(c>5):\n",
        "    stop = input(\"Stop?\")\n",
        "    if stop == 'y':\n",
        "      break"
      ],
      "metadata": {
        "id": "5Y0pfXNFr-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hand_image =  prob[hand_image[0][1]:hand_image[1][1], hand_image[0][0]:hand_image[1][0]] #capturing hand on probability map\n",
        "hand_image = cv2.resize(hand_image, (16,16)) #resizing image to same size as the one used in our model\n",
        "hand_image = hand_image.reshape((1,256)) #reshaping to fit data used in our mode"
      ],
      "metadata": {
        "id": "VM31QpnF5l5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading models\n",
        "\n",
        "After detecting the hand, we need to load the second actor of this task: \"Model\". Since we have made 3 different models, they will be loaded and tested all. "
      ],
      "metadata": {
        "id": "DYB1E_zcchI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model1 = load_model('model1.json',\"model1_weights.h5\") #loading model 1\n",
        "loaded_model2 = load_model('model2.json',\"model2_weights.h5\") #loading model 2\n",
        "loaded_model3 = load_model('model3.json',\"model3_weights.h5\") #loading model 3"
      ],
      "metadata": {
        "id": "oqQN8Hje0C4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on detected hand\n",
        "\n",
        "Now, all the element are here (hand_image + model), the prediction is made as follow."
      ],
      "metadata": {
        "id": "eiUv03NFclgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The predicted letter with model 1 is :', predict(hand_image,loaded_model1))\n",
        "print('The predicted letter with model 2 is :', predict(hand_image,loaded_model2))\n",
        "print('The predicted letter with model 3 is :', predict(hand_image,loaded_model3))"
      ],
      "metadata": {
        "id": "lfTaLaA6qoFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
